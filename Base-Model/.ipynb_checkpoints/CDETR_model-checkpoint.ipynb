{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import List, Optional\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw #sample data generation\n",
    "import io\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_, constant_, xavier_normal_\n",
    "import torchvision\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models._utils import IntermediateLayerGetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panoptic API Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2id(color):\n",
    "    if isinstance(color, np.ndarray) and len(color.shape) == 3:\n",
    "        if color.dtype == np.uint8:\n",
    "            color = color.astype(np.int32)\n",
    "        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n",
    "    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n",
    "\n",
    "\n",
    "def id2rgb(id_map):\n",
    "    if isinstance(id_map, np.ndarray):\n",
    "        id_map_copy = id_map.copy()\n",
    "        rgb_shape = tuple(list(id_map.shape) + [3])\n",
    "        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)\n",
    "        for i in range(3):\n",
    "            rgb_map[..., i] = id_map_copy % 256\n",
    "            id_map_copy //= 256\n",
    "        return rgb_map\n",
    "    color = []\n",
    "    for _ in range(3):\n",
    "        color.append(id_map % 256)\n",
    "        id_map //= 256\n",
    "    return color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "- **PositionEmbeddingSine**: It now calculates the sinusoidal embeddings based on the positions of elements in the image grid, suitable for transformers.\n",
    "- **PositionEmbeddingLearned**: Adjusted to initialize weights more robustly and provided parameters for max dimensions to accommodate different image sizes.\n",
    "- **build_position_encoding**: This function now supports creating either 'sine' or 'learned' embeddings based on the input arguments, making it flexible for different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"Sinusoidal Position Embedding for image-based inputs.\"\"\"\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        self.scale = scale if scale is not None else 2 * math.pi\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.mask is not None, \"Mask cannot be None\"\n",
    "        mask = x.mask\n",
    "        not_mask = ~mask\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "    \n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.tensors.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"Learned Position Embedding for image-based inputs.\"\"\"\n",
    "    def __init__(self, num_pos_feats=256, max_height=50, max_width=50):\n",
    "        super().__init__()\n",
    "        self.row_embed = nn.Embedding(max_height, num_pos_feats)\n",
    "        self.col_embed = nn.Embedding(max_width, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight, -0.05, 0.05)\n",
    "        nn.init.uniform_(self.col_embed.weight, -0.05, 0.05)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.mask is not None, \"Mask cannot be None\"\n",
    "        h, w = x.tensors.shape[-2:]\n",
    "        i = torch.arange(w, device=x.tensors.device)\n",
    "        j = torch.arange(h, device=x.tensors.device)\n",
    "        x_emb = self.col_embed(i)\n",
    "        y_emb = self.row_embed(j)\n",
    "        pos = torch.cat([x_emb.unsqueeze(0).repeat(h, 1, 1), y_emb.unsqueeze(1).repeat(1, w, 1)], dim=-1)\n",
    "        pos = pos.permute(2, 0, 1).unsqueeze(0).repeat(x.tensors.size(0), 1, 1, 1)\n",
    "        \n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_position_encoding(hidden_dim, position_embedding='sine', max_height=50, max_width=50):\n",
    "    \"\"\"Factory function to build position encoding.\"\"\"\n",
    "    num_pos_feats = hidden_dim // 2\n",
    "    if position_embedding == 'sine':\n",
    "        return PositionEmbeddingSine(num_pos_feats, normalize=True)\n",
    "    elif position_embedding == 'learned':\n",
    "        return PositionEmbeddingLearned(num_pos_feats, max_height, max_width)\n",
    "    else:\n",
    "        raise ValueError(f\"Not supported position embedding type {position_embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "This refactored code maintains the same overall structure but simplifies the definition and initialization of the backbone and its components. The `FrozenBatchNorm2d` function is streamlined by using a regular `BatchNorm2d` and setting the gradient flags to `False`. The `BackboneBase` is merged and simplified into `SimplifiedBackbone`, and the functionality of handling different returning layers and feature channels is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frozen_batch_norm2d(num_features, eps=1e-5):\n",
    "    \"\"\"Creates a frozen batch normalization layer.\"\"\"\n",
    "    layer = nn.BatchNorm2d(num_features, affine=True)\n",
    "    layer.weight.requires_grad = False\n",
    "    layer.bias.requires_grad = False\n",
    "    layer.running_mean.requires_grad = False\n",
    "    layer.running_var.requires_grad = False\n",
    "    layer.eps = eps\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device):\n",
    "        cast_tensor = self.tensors.to(device)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            assert mask is not None\n",
    "            cast_mask = mask.to(device)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBackbone(nn.Module):\n",
    "    \"\"\" Simplified backbone model for feature extraction. \"\"\"\n",
    "    def __init__(self, model_name, train_layers=False, num_channels=2048, return_interm_layers=False):\n",
    "        super().__init__()\n",
    "        backbone = resnet50(weights=ResNet50_Weights.DEFAULT if not train_layers else None, norm_layer=lambda num_features: frozen_batch_norm2d(num_features))\n",
    "        for name, parameter in backbone.named_parameters():\n",
    "            if not train_layers or all(not name.startswith(f'layer{i}') for i in [2, 3, 4]):\n",
    "                parameter.requires_grad = False\n",
    "        \n",
    "        # Setup return_layers to map the desired layer to '0'\n",
    "        return_layers = {'layer4': '0'} if not return_interm_layers else {f'layer{i}': str(i-1) for i in range(1, 5)}\n",
    "        \n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, nested_tensor):\n",
    "        tensors = nested_tensor.tensors\n",
    "        mask = nested_tensor.mask\n",
    "        xs = self.body(tensors)\n",
    "        \n",
    "        return {name: NestedTensor(x, mask) for name, x in xs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneWithPositionEmbedding(nn.Sequential):\n",
    "    def __init__(self, backbone, position_embedding):\n",
    "        super().__init__(backbone, position_embedding)\n",
    "        self.num_channels = backbone.num_channels\n",
    "\n",
    "    def forward(self, nested_tensor):\n",
    "        xs = self[0](nested_tensor)  # Calls forward on backbone\n",
    "        pos = {name: self[1](x) for name, x in xs.items()}  # Generate position embeddings\n",
    "        return xs, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_refactored_backbone(model_name, train_layers, num_channels, return_interm_layers, hidden_dim, position_embedding_type):\n",
    "    \"\"\"Builds a backbone with integrated position encoding.\"\"\"\n",
    "    position_embedding = build_position_encoding(hidden_dim, position_embedding_type)\n",
    "    backbone = SimplifiedBackbone(model_name, train_layers, num_channels, return_interm_layers)\n",
    "    return BackboneWithPositionEmbedding(backbone, position_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "This version of the Hungarian matcher will compute matching costs using classification scores, point distances, and optionally GIoU scores (if applicable). However, since the original paper discusses a KMO-based matcher, and your model doesn't include explicit GIoU computation for bounding boxes, we'll focus only on classification and point distances for simplicity.\n",
    "\n",
    "\n",
    "1. **Logits to Probabilities**: The matcher uses `softmax` to convert the logits to probabilities. This is crucial for calculating the classification cost as negative log likelihood, which is more stable and interpretable.\n",
    "2. **Cost Calculation**: The matcher combines classification and point costs linearly using specified weights. It supports batch processing where each element in the batch has its own set of targets and predictions.\n",
    "3. **Linear Sum Assignment**: This uses the `linear_sum_assignment` from `SciPy`, which directly finds the minimum cost matching. It's applied batch-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"Computes an assignment between predictions and ground truth targets.\"\"\"\n",
    "    def __init__(self, cost_class: float = 1.0, cost_point: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_point = cost_point\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            outputs: Dict containing at least 'pred_logits' and 'pred_points'.\n",
    "                - 'pred_logits': Tensor of shape [batch_size, num_queries, num_classes]\n",
    "                - 'pred_points': Tensor of shape [batch_size, num_queries, 4] (for bounding box points)\n",
    "            targets: List of dictionaries containing:\n",
    "                - 'labels': Tensor of shape [num_target_points]\n",
    "                - 'points': Tensor of shape [num_target_points, 4]\n",
    "\n",
    "        Returns:\n",
    "            List of tuples for each batch element, containing:\n",
    "            - Index of selected predictions.\n",
    "            - Index of corresponding selected targets.\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs['pred_logits'].shape[:2]\n",
    "        out_prob = outputs['pred_logits'].softmax(-1)  # Convert logits to probabilities\n",
    "        out_points = outputs['pred_points'].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Concatenate all targets across the batch\n",
    "        tgt_points = torch.cat([t['points'] for t in targets]).to(out_points.device)\n",
    "        tgt_labels = torch.cat([t['labels'] for t in targets])\n",
    "\n",
    "        # Compute classification cost using negative log likelihood\n",
    "        cost_class = -out_prob[:, tgt_labels].flatten(0, 1)\n",
    "\n",
    "        # Compute L1 cost between predicted points and target points\n",
    "        cost_point = torch.cdist(out_points, tgt_points, p=1)\n",
    "\n",
    "        # Combine costs\n",
    "        C = self.cost_class * cost_class + self.cost_point * cost_point\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        # Compute assignment for each batch element\n",
    "        indices = [linear_sum_assignment(c) for c in C]\n",
    "        return [(torch.as_tensor(i, dtype=torch.long), torch.as_tensor(j, dtype=torch.long)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matcher(cost_class=1.0, cost_point=1.0):\n",
    "    return HungarianMatcher(cost_class, cost_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "This `MultiheadAttention`:\n",
    "- Directly uses the input dimensions of query, key, and value without additional projection, simplifying the architecture.\n",
    "- Uses scaled dot-product attention mechanism, following the principle outlined in \"Attention is All You Need\".\n",
    "- Provides dropout for regularization and a final linear projection to match the output dimensions to the input dimensions.\n",
    "- Is flexible with regard to whether it returns attention weights, allowing for easier debugging or further manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"Custom implementation of MultiheadAttention to support different dimensions\n",
    "    for query, key, and value without separate projection matrices for each.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads  # Make sure this is set\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameter initialization\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        xavier_uniform_(self.out_proj.weight)\n",
    "        if self.out_proj.bias is not None:\n",
    "            constant_(self.out_proj.bias, 0)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None):\n",
    "        \"\"\" Forward pass for custom multihead attention.\n",
    "        Assumes L = target sequence length, S = source sequence length, N = batch size, E = embedding dimension.\n",
    "        \"\"\"\n",
    "        tgt_len, bsz, embed_dim = query.size()\n",
    "        assert query.size() == key.size() == value.size(), \"Query, Key and Value must be of the same size\"\n",
    "\n",
    "        # Scale query for dot product attention\n",
    "        scaling = self.head_dim ** -0.5\n",
    "        query = query * scaling\n",
    "\n",
    "        # Calculate Q, K, V\n",
    "        q = query.reshape(tgt_len, bsz * self.num_heads, self.head_dim)\n",
    "        k = key.reshape(-1, bsz * self.num_heads, self.head_dim)\n",
    "        v = value.reshape(-1, bsz * self.num_heads, self.head_dim)\n",
    "\n",
    "        # Dot product of Q and K (transpose)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, -1)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf')\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * self.num_heads, tgt_len, -1)\n",
    "\n",
    "        # Apply softmax and dropout on attention weights\n",
    "        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "        attn_output_weights = self.dropout_layer(attn_output_weights)\n",
    "\n",
    "        # Multiply weights by V\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "\n",
    "        # Transpose and reshape to bring back to original dimensions\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "\n",
    "        # Apply final linear projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        if need_weights:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, -1)\n",
    "            attn_output_weights = attn_output_weights.sum(dim=1) / self.num_heads\n",
    "        else:\n",
    "            attn_output_weights = None\n",
    "\n",
    "        return attn_output, attn_output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    \"\"\"Create N identical layers by deep copying the given module.\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    elif activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(\"Activation function {} is not supported\".format(activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, layer, n_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(layer, n_layers)\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.embed_dim)\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        src = self.norm(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, layer, n_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(layer, n_layers)\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.embed_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        tgt = self.norm(tgt)\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, pos_embed):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.nhead = encoder.layers[0].self_attn.num_heads\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.pos_embed = pos_embed\n",
    "        self.d_model = encoder.layers[0].self_attn.embed_dim  # Assuming all layers have the same embed_dim\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        memory = self.encoder(self.pos_embed(self.src_embed(src)), src_mask, src_key_padding_mask)\n",
    "        output = self.decoder(self.pos_embed(self.tgt_embed(tgt)), memory, tgt_mask, None, tgt_key_padding_mask, None)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation='relu'):\n",
    "    \"\"\"Function to build the transformer model.\"\"\"\n",
    "    encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "    encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "    \n",
    "    decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "    decoder = TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "    \n",
    "    src_embed = nn.Embedding(num_embeddings=10, embedding_dim=d_model)  # Placeholder for actual embedding logic\n",
    "    tgt_embed = nn.Embedding(num_embeddings=10, embedding_dim=d_model)  # Placeholder for actual embedding logic\n",
    "    pos_encoder = PositionalEncoding(d_model=d_model)\n",
    "    \n",
    "    return TransformerModel(encoder, decoder, src_embed, tgt_embed, pos_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" Multi-layer Perceptron (MLP) with variable number of layers \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMap(nn.Module):\n",
    "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        nn.init.zeros_(self.k_linear.bias)\n",
    "        nn.init.zeros_(self.q_linear.bias)\n",
    "        nn.init.xavier_uniform_(self.k_linear.weight)\n",
    "        nn.init.xavier_uniform_(self.q_linear.weight)\n",
    "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, q, k, mask: Optional[Tensor] = None):\n",
    "        q = self.q_linear(q)\n",
    "        k = F.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
    "        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
    "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", qh * self.normalize_fact, kh)\n",
    "\n",
    "        if mask is not None:\n",
    "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "        weights = F.softmax(weights.flatten(2), dim=-1).view(weights.size())\n",
    "        weights = self.dropout(weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalMaskHead(nn.Module):\n",
    "    def __init__(self, dim, fpn_dims, context_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n",
    "        self.lay1 = nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(8, dim)\n",
    "        self.lay2 = nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(8, inter_dims[1])\n",
    "        self.lay3 = nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
    "        self.gn3 = nn.GroupNorm(8, inter_dims[2])\n",
    "        self.lay4 = nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
    "        self.gn4 = nn.GroupNorm(8, inter_dims[3])\n",
    "        self.lay5 = nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
    "        self.gn5 = nn.GroupNorm(8, inter_dims[4])\n",
    "        self.out_lay = nn.Conv2d(inter_dims[4], 1, 3, padding=1)\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.adapter1 = nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
    "        self.adapter2 = nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
    "        self.adapter3 = nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):\n",
    "        x = torch.cat([self._expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)\n",
    "\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = self._expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = self._expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = self._expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.out_lay(x)\n",
    "        return x\n",
    "\n",
    "    def _expand(self, tensor, length: int):\n",
    "        return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "    if tensor_list[0].ndim == 3:\n",
    "        max_size = [max(s) for s in zip(*[img.shape for img in tensor_list])]\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "            m[: img.shape[1], :img.shape[2]] = False\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return NestedTensor(tensor, mask)\n",
    "\n",
    "\n",
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device):\n",
    "        cast_tensor = self.tensors.to(device)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            cast_mask = mask.to(device)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone, transformer, num_queries, aux_loss=False):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.transformer = transformer\n",
    "        self.num_queries = num_queries\n",
    "        self.aux_loss = aux_loss\n",
    "\n",
    "        hidden_dim, nheads = transformer.d_model, transformer.nhead\n",
    "        self.bbox_attention = AttentionMap(hidden_dim, hidden_dim, nheads)\n",
    "        self.mask_head = ConvolutionalMaskHead(hidden_dim + nheads, [1024, 512, 256], hidden_dim)\n",
    "\n",
    "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "        self.class_embed = nn.Linear(hidden_dim, 91)\n",
    "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "\n",
    "    def forward(self, samples):\n",
    "        if isinstance(samples, (list, torch.Tensor)):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.backbone(samples)\n",
    "\n",
    "        src, mask = features[-1].decompose()\n",
    "        src_proj = self.input_proj(src)\n",
    "        hs, memory = self.transformer(src_proj, mask, self.query_embed.weight, pos[-1])\n",
    "\n",
    "        outputs_class = self.class_embed(hs)\n",
    "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
    "        out = {\"pred_logits\": outputs_class[-1], \"pred_boxes\": outputs_coord[-1]}\n",
    "        if self.aux_loss:\n",
    "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
    "\n",
    "        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)\n",
    "        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])\n",
    "        outputs_seg_masks = seg_masks.view(samples.tensors.shape[0], self.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
    "\n",
    "        out[\"pred_masks\"] = outputs_seg_masks\n",
    "        return out\n",
    "\n",
    "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
    "        return [{\"pred_logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcess(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
    "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
    "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
    "        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n",
    "        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
    "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
    "\n",
    "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
    "            img_h, img_w = t[0], t[1]\n",
    "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
    "            results[i][\"masks\"] = F.interpolate(results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\").byte()\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def masks_to_boxes(masks):\n",
    "    if masks.numel() == 0:\n",
    "        return torch.zeros((0, 4), device=masks.device)\n",
    "\n",
    "    h, w = masks.shape[-2:]\n",
    "    y = torch.arange(0, h, dtype=torch.float)\n",
    "    x = torch.arange(0, w, dtype=torch.float)\n",
    "    y, x = torch.meshgrid(y, x)\n",
    "\n",
    "    x_mask = (masks * x.unsqueeze(0))\n",
    "    x_max = x_mask.flatten(1).max(-1)[0]\n",
    "    x_min = x_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
    "\n",
    "    y_mask = (masks * y.unsqueeze(0))\n",
    "    y_max = y_mask.flatten(1).max(-1)[0]\n",
    "    y_min = y_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
    "\n",
    "    return torch.stack([x_min, y_min, x_max, y_max], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessPanoptic(nn.Module):\n",
    "    def __init__(self, is_thing_map, threshold=0.85):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.is_thing_map = is_thing_map\n",
    "\n",
    "    def forward(self, outputs, processed_sizes, target_sizes=None):\n",
    "        if target_sizes is None:\n",
    "            target_sizes = processed_sizes\n",
    "        assert len(processed_sizes) == len(target_sizes)\n",
    "        out_logits, raw_masks, raw_boxes = outputs[\"pred_logits\"], outputs[\"pred_masks\"], outputs[\"pred_boxes\"]\n",
    "        assert len(out_logits) == len(raw_masks) == len(target_sizes)\n",
    "        preds = []\n",
    "\n",
    "        def to_tuple(tup):\n",
    "            if isinstance(tup, tuple):\n",
    "                return tup\n",
    "            return tuple(tup.cpu().tolist())\n",
    "\n",
    "        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes):\n",
    "            scores, labels = cur_logits.softmax(-1).max(-1)\n",
    "            keep = labels.ne(outputs[\"pred_logits\"].shape[-1] - 1) & (scores > self.threshold)\n",
    "            cur_scores, cur_classes = cur_logits.softmax(-1).max(-1)\n",
    "            cur_scores = cur_scores[keep]\n",
    "            cur_classes = cur_classes[keep]\n",
    "            cur_masks = cur_masks[keep]\n",
    "            cur_masks = F.interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n",
    "            cur_boxes = box_cxcywh_to_xyxy(cur_boxes[keep])\n",
    "\n",
    "            h, w = cur_masks.shape[-2:]\n",
    "            assert len(cur_boxes) == len(cur_classes)\n",
    "\n",
    "            cur_masks = cur_masks.flatten(1)\n",
    "            stuff_equiv_classes = defaultdict(lambda: [])\n",
    "            for k, label in enumerate(cur_classes):\n",
    "                if not self.is_thing_map[label.item()]:\n",
    "                    stuff_equiv_classes[label.item()].append(k)\n",
    "\n",
    "            def get_ids_area(masks, scores, dedup=False):\n",
    "                m_id = masks.transpose(0, 1).softmax(-1)\n",
    "                if m_id.shape[-1] == 0:\n",
    "                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n",
    "                else:\n",
    "                    m_id = m_id.argmax(-1).view(h, w)\n",
    "                if dedup:\n",
    "                    for equiv in stuff_equiv_classes.values():\n",
    "                        if len(equiv) > 1:\n",
    "                            for eq_id in equiv:\n",
    "                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n",
    "                final_h, final_w = to_tuple(target_size)\n",
    "                seg_img = Image.fromarray(id2rgb(m_id.view(h, w).cpu().numpy()))\n",
    "                seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)\n",
    "                np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes())).view(final_h, final_w, 3).numpy()\n",
    "                m_id = torch.from_numpy(rgb2id(np_seg_img))\n",
    "                area = []\n",
    "                for i in range(len(scores)):\n",
    "                    area.append(m_id.eq(i).sum().item())\n",
    "                return area, seg_img\n",
    "\n",
    "            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n",
    "            if cur_classes.numel() > 0:\n",
    "                while True:\n",
    "                    filtered_small = torch.as_tensor([area[i] <= 4 for i, c in enumerate(cur_classes)], dtype=torch.bool, device=keep.device)\n",
    "                    if filtered_small.any().item():\n",
    "                        cur_scores = cur_scores[~filtered_small]\n",
    "                        cur_classes = cur_classes[~filtered_small]\n",
    "                        cur_masks = cur_masks[~filtered_small]\n",
    "                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)\n",
    "\n",
    "            segments_info = []\n",
    "            for i, a in enumerate(area):\n",
    "                cat = cur_classes[i].item()\n",
    "                segments_info.append({\"id\": i, \"isthing\": self.is_thing_map[cat], \"category_id\": cat, \"area\": a})\n",
    "            del cur_classes\n",
    "\n",
    "            with io.BytesIO() as out:\n",
    "                seg_img.save(out, format=\"PNG\")\n",
    "                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n",
    "            preds.append(predictions)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRsegm(nn.Module):\n",
    "    def __init__(self, detr, freeze_detr=False):\n",
    "        super().__init__()\n",
    "        self.detr = detr\n",
    "\n",
    "        if freeze_detr:\n",
    "            for p in self.detr.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        hidden_dim = self.detr.transformer.d_model\n",
    "        nheads = self.detr.transformer.nhead  # This assumes nhead is correctly exposed\n",
    "        self.bbox_attention = AttentionMap(hidden_dim, hidden_dim, nheads)\n",
    "        self.mask_head = ConvolutionalMaskHead(hidden_dim + nheads, [1024, 512, 256], hidden_dim)\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        if isinstance(samples, (list, torch.Tensor)):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.detr.backbone(samples)\n",
    "\n",
    "        # Here, ensure '0' or the correct key is used\n",
    "        bs = features['0'].tensors.shape[0]\n",
    "\n",
    "        src, mask = features['0'].decompose()\n",
    "        assert mask is not None\n",
    "        src_proj = self.detr.input_proj(src)\n",
    "        hs, memory = self.detr.transformer(src_proj, mask, self.detr.query_embed.weight, pos[-1])\n",
    "\n",
    "        outputs_class = self.detr.class_embed(hs)\n",
    "        outputs_coord = self.detr.point_embed(hs).sigmoid()\n",
    "        out = {\"pred_logits\": outputs_class[-1], \"pred_boxes\": outputs_coord[-1]}\n",
    "        if self.detr.aux_loss:\n",
    "            out['aux_outputs'] = self.detr._set_aux_loss(outputs_class, outputs_coord)\n",
    "\n",
    "        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)\n",
    "\n",
    "        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])\n",
    "        outputs_seg_masks = seg_masks.view(bs, self.detr.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
    "\n",
    "        out[\"pred_masks\"] = outputs_seg_masks\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessSegm(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
    "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
    "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
    "        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n",
    "        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
    "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
    "\n",
    "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
    "            img_h, img_w = t[0], t[1]\n",
    "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
    "            results[i][\"masks\"] = F.interpolate(\n",
    "                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n",
    "            ).byte()\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional DETR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def box_cxcywh_to_xyxy(x):\n",
    "#     \"\"\" Convert box format from [center_x, center_y, width, height] to [x_min, y_min, x_max, y_max] \"\"\"\n",
    "#     x_c, y_c, w, h = x.unbind(-1)\n",
    "#     b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "#     return torch.stack(b, dim=-1)\n",
    "\n",
    "def inverse_sigmoid(x, eps=1e-5):\n",
    "    \"\"\" Convert probabilities to logits with safe clamping \"\"\"\n",
    "    x = x.clamp(min=0, max=1)\n",
    "    x1 = x.clamp(min=eps)\n",
    "    x2 = (1 - x).clamp(min=eps)\n",
    "    return torch.log(x1 / x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dist_avail_and_initialized():\n",
    "    if not torch.distributed.is_available():\n",
    "        return False\n",
    "    if not torch.distributed.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return torch.distributed.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    if target.numel() == 0:\n",
    "        return [torch.zeros([], device=output.device)]\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(inputs, targets, num_boxes):\n",
    "    inputs = inputs.sigmoid().flatten(1)\n",
    "    numerator = 2 * (inputs * targets).sum(1)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
    "    return loss.sum() / num_boxes\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    return loss.mean(1).sum() / num_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional DETR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDETR(nn.Module):\n",
    "    \"\"\" Conditional DETR module for object detection \"\"\"\n",
    "    def __init__(self, backbone, transformer, num_classes, num_queries, channel_point, aux_loss=False):\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "        self.transformer = transformer\n",
    "        hidden_dim = transformer.d_model\n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes)\n",
    "        self.point_embed = MLP(hidden_dim, hidden_dim, channel_point, 3)\n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
    "        self.backbone = backbone\n",
    "        self.aux_loss = aux_loss\n",
    "\n",
    "        # Initialize biases for focal loss\n",
    "        prior_prob = 0.01\n",
    "        bias_value = -math.log((1 - prior_prob) / prior_prob)\n",
    "        self.class_embed.bias.data = torch.ones(num_classes) * bias_value\n",
    "\n",
    "        # Initialize point embedding layers\n",
    "        nn.init.constant_(self.point_embed.layers[-1].weight.data, 0)\n",
    "        nn.init.constant_(self.point_embed.layers[-1].bias.data, 0)\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        if isinstance(samples, (list, torch.Tensor)):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.backbone(samples)\n",
    "\n",
    "        src, mask = features[-1].decompose()\n",
    "        assert mask is not None\n",
    "        hs, reference = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])\n",
    "\n",
    "        reference_before_sigmoid = inverse_sigmoid(reference)\n",
    "        outputs_coords = []\n",
    "        for lvl in range(hs.shape[0]):\n",
    "            tmp = self.point_embed(hs[lvl])\n",
    "            tmp[..., :2] += reference_before_sigmoid\n",
    "            outputs_coord = tmp.sigmoid()\n",
    "            outputs_coords.append(outputs_coord)\n",
    "        outputs_coord = torch.stack(outputs_coords)\n",
    "\n",
    "        outputs_class = self.class_embed(hs)\n",
    "        out = {'pred_logits': outputs_class[-1], 'pred_points': outputs_coord[-1]}\n",
    "        if self.aux_loss:\n",
    "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
    "        return out\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
    "        return [{'pred_logits': a, 'pred_points': b}\n",
    "                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Criterion for Loss Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" Computes the loss for Conditional DETR \"\"\"\n",
    "    def __init__(self, num_classes, matcher, weight_dict, focal_alpha, losses):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.losses = losses\n",
    "        self.focal_alpha = focal_alpha\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_points, log=True):\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)]).cuda()\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        target_classes_onehot = torch.zeros([src_logits.shape[0], src_logits.shape[1], src_logits.shape[2] + 1],\n",
    "                                            dtype=src_logits.dtype, layout=src_logits.layout, device=src_logits.device)\n",
    "        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n",
    "\n",
    "        target_classes_onehot = target_classes_onehot[:, :, :-1]\n",
    "        loss_ce = sigmoid_focal_loss(src_logits, target_classes_onehot, num_points, alpha=self.focal_alpha, gamma=2) * \\\n",
    "                  src_logits.shape[1]\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        if log:\n",
    "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_points):\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_points(self, outputs, targets, indices, num_points):\n",
    "        assert 'pred_points' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_points = outputs['pred_points'][idx]\n",
    "        target_points = torch.cat([t['points'][i] for t, (_, i) in zip(targets, indices)], dim=0).cuda()\n",
    "\n",
    "        loss_point = F.l1_loss(src_points, target_points, reduction='none')\n",
    "        losses = {'loss_point': loss_point.sum() / num_points}\n",
    "        return losses\n",
    "\n",
    "    def loss_masks(self, outputs, targets, indices, num_points):\n",
    "        assert \"pred_masks\" in outputs\n",
    "\n",
    "        src_idx = self._get_src_permutation_idx(indices)\n",
    "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
    "        src_masks = outputs[\"pred_masks\"]\n",
    "        src_masks = src_masks[src_idx]\n",
    "        masks = [t[\"masks\"] for t in targets]\n",
    "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
    "        target_masks = target_masks.to(src_masks)\n",
    "        target_masks = target_masks[tgt_idx]\n",
    "\n",
    "        src_masks = F.interpolate(src_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        src_masks = src_masks[:, 0].flatten(1)\n",
    "\n",
    "        target_masks = target_masks.flatten(1)\n",
    "        target_masks = target_masks.view(src_masks.shape)\n",
    "        losses = {\n",
    "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_points),\n",
    "            \"loss_dice\": dice_loss(src_masks, target_masks, num_points),\n",
    "        }\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_points, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'points': self.loss_points,\n",
    "            'masks': self.loss_masks\n",
    "        }\n",
    "        assert loss in loss_map, f'Unsupported loss: {loss}'\n",
    "        return loss_map[loss](outputs, targets, indices, num_points, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        num_points = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_points = torch.as_tensor([num_points], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        if is_dist_avail_and_initialized():\n",
    "            torch.distributed.all_reduce(num_points)\n",
    "        num_points = torch.clamp(num_points / get_world_size(), min=1).item()\n",
    "\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_points))\n",
    "\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
    "                indices = self.matcher(aux_outputs, targets)\n",
    "                for loss in self.losses:\n",
    "                    if loss == 'masks':\n",
    "                        continue\n",
    "                    kwargs = {}\n",
    "                    if loss == 'labels':\n",
    "                        kwargs = {'log': False}\n",
    "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_points, **kwargs)\n",
    "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcess(nn.Module):\n",
    "    \"\"\" Convert model's output to the format expected by the COCO API \"\"\"\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        out_logits, out_points = outputs['pred_logits'], outputs['pred_points']\n",
    "        assert len(out_logits) == len(target_sizes)\n",
    "        assert target_sizes.shape[1] == 2\n",
    "\n",
    "        prob = out_logits.sigmoid()\n",
    "        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n",
    "        scores = topk_values\n",
    "        topk_points = topk_indexes // out_logits.shape[2]\n",
    "        labels = topk_indexes % out_logits.shape[2]\n",
    "        points = box_cxcywh_to_xyxy(out_points)\n",
    "        points = torch.gather(points, 1, topk_points.unsqueeze(-1).repeat(1, 1, 4))\n",
    "\n",
    "        img_h, img_w = target_sizes.unbind(1)\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
    "        points = points * scale_fct[:, None, :]\n",
    "\n",
    "        results = [{'scores': s, 'labels': l, 'points': b} for s, l, b in zip(scores, labels, points)]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conditional_detr(args):\n",
    "    \"\"\"\n",
    "    Builds the Conditional DETR model.\n",
    "    \n",
    "    Arguments:\n",
    "    - args: configuration arguments for building the model.\n",
    "    \n",
    "    Returns:\n",
    "    - model: the Conditional DETR model.\n",
    "    - criterion: loss function used for training.\n",
    "    - postprocessors: post-processing modules for transforming model outputs.\n",
    "    \"\"\"\n",
    "    num_classes = 2 if args.dataset_file != 'coco' else 91\n",
    "    if args.dataset_file == \"coco_panoptic\":\n",
    "        num_classes = 250\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # Build backbone using your refactored method\n",
    "    backbone = build_refactored_backbone(\n",
    "        model_name=args.backbone,\n",
    "        train_layers=args.lr_backbone > 0,\n",
    "        num_channels=args.num_channels,\n",
    "        return_interm_layers=args.masks,\n",
    "        hidden_dim=args.hidden_dim,\n",
    "        position_embedding_type=args.position_embedding\n",
    "    )\n",
    "\n",
    "    # Build transformer using your provided method\n",
    "    transformer = build_transformer(\n",
    "        d_model=args.hidden_dim,\n",
    "        nhead=args.nheads,\n",
    "        num_encoder_layers=args.enc_layers,\n",
    "        num_decoder_layers=args.dec_layers,\n",
    "        dim_feedforward=args.dim_feedforward,\n",
    "        dropout=args.dropout,\n",
    "        activation='relu'  # or use args.activation if provided\n",
    "    )\n",
    "\n",
    "    # Create the ConditionalDETR model\n",
    "    model = ConditionalDETR(\n",
    "        backbone=backbone,\n",
    "        transformer=transformer,\n",
    "        num_classes=num_classes,\n",
    "        num_queries=args.num_queries,\n",
    "        channel_point=args.channel_point,\n",
    "        aux_loss=args.aux_loss,\n",
    "    )\n",
    "\n",
    "    # Wrap with DETRsegm if masks are enabled\n",
    "    if args.masks:\n",
    "        model = DETRsegm(model, freeze_detr=(args.frozen_weights is not None))\n",
    "\n",
    "    # Build matcher using your provided method\n",
    "    matcher = build_matcher(cost_class=args.cost_class, cost_point=args.cost_point)\n",
    "    \n",
    "    # Define weight dictionary for losses\n",
    "    weight_dict = {'loss_ce': args.cls_loss_coef, 'loss_point': args.point_loss_coef}\n",
    "    weight_dict['loss_giou'] = args.giou_loss_coef\n",
    "    if args.masks:\n",
    "        weight_dict[\"loss_mask\"] = args.mask_loss_coef\n",
    "        weight_dict[\"loss_dice\"] = args.dice_loss_coef\n",
    "\n",
    "    # Add auxiliary loss weights if aux_loss is enabled\n",
    "    if args.aux_loss:\n",
    "        aux_weight_dict = {}\n",
    "        for i in range(args.dec_layers - 1):\n",
    "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
    "        weight_dict.update(aux_weight_dict)\n",
    "\n",
    "    # Define the losses to be used\n",
    "    losses = ['labels', 'points', 'cardinality']\n",
    "    if args.masks:\n",
    "        losses += [\"masks\"]\n",
    "\n",
    "    # Create the criterion (loss function)\n",
    "    criterion = SetCriterion(\n",
    "        num_classes=num_classes, \n",
    "        matcher=matcher, \n",
    "        weight_dict=weight_dict,\n",
    "        focal_alpha=args.focal_alpha, \n",
    "        losses=losses\n",
    "    )\n",
    "    criterion.to(device)\n",
    "\n",
    "    # Define postprocessors\n",
    "    postprocessors = {'point': PostProcess()}\n",
    "    if args.masks:\n",
    "        postprocessors['segm'] = PostProcessSegm()\n",
    "        if args.dataset_file == \"coco_panoptic\":\n",
    "            is_thing_map = {i: i <= 90 for i in range(201)}\n",
    "            postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
    "\n",
    "    return model, criterion, postprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_samples, image_size=(256, 256), num_objects=5, seed=54):\n",
    "    \"\"\"\n",
    "    Generates synthetic images and annotations for testing.\n",
    "    \n",
    "    Args:\n",
    "    - num_samples: Number of samples to generate.\n",
    "    - image_size: Size of the synthetic image (height, width).\n",
    "    - num_objects: Number of objects per image.\n",
    "    \n",
    "    Returns:\n",
    "    - images: List of synthetic images as Tensors.\n",
    "    - targets: List of dictionaries containing 'labels' and 'points'.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    rng = np.random.default_rng(seed)  # Create a random number generator instance\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        image = Image.new('RGB', image_size, (255, 255, 255))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        sample_labels = []\n",
    "        sample_points = []\n",
    "        sample_masks = []\n",
    "        \n",
    "        for obj_id in range(num_objects):\n",
    "            # Random object size and position\n",
    "            w, h = rng.integers(10, 40, size=2)\n",
    "            x = rng.integers(0, image_size[1] - w)\n",
    "            y = rng.integers(0, image_size[0] - h)\n",
    "            \n",
    "            # Draw rectangle on the image\n",
    "            draw.rectangle([x, y, x+w, y+h], outline='black', fill=(rng.integers(255), rng.integers(255), rng.integers(255)))\n",
    "            \n",
    "            # Generate label and point\n",
    "            sample_labels.append(torch.tensor([obj_id % 2]))  # Label alternating between 0 and 1 for binary classification\n",
    "            sample_points.append(torch.tensor([x + w/2, y + h/2, w, h], dtype=torch.float32))\n",
    "            \n",
    "            # Generate a binary mask\n",
    "            mask = torch.zeros(image_size, dtype=torch.uint8)\n",
    "            mask[y:y+h, x:x+w] = 1\n",
    "            sample_masks.append(mask)\n",
    "        \n",
    "        images.append(torch.tensor(np.array(image).transpose(2, 0, 1), dtype=torch.float32) / 255.0)\n",
    "        \n",
    "        targets.append({\n",
    "            'labels': torch.stack(sample_labels),\n",
    "            'points': torch.stack(sample_points),\n",
    "            'masks': torch.stack(sample_masks)\n",
    "        })\n",
    "    \n",
    "    return images, targets\n",
    "\n",
    "# Now you can call the generate_synthetic_data function without warnings\n",
    "synthetic_images, synthetic_targets = generate_synthetic_data(num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    dataset_file = 'test'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    backbone = 'resnet50'\n",
    "    lr_backbone = 0.0\n",
    "    num_channels = 2048\n",
    "    return_interm_layers = False\n",
    "    hidden_dim = 256\n",
    "    position_embedding = 'sine'\n",
    "    nheads = 8\n",
    "    enc_layers = 6\n",
    "    dec_layers = 6\n",
    "    dim_feedforward = 1024\n",
    "    dropout = 0.1\n",
    "    num_queries = 10\n",
    "    channel_point = 4\n",
    "    aux_loss = False\n",
    "    masks = True\n",
    "    frozen_weights = None\n",
    "    cls_loss_coef = 1.0\n",
    "    point_loss_coef = 1.0\n",
    "    giou_loss_coef = 1.0\n",
    "    mask_loss_coef = 1.0\n",
    "    dice_loss_coef = 1.0\n",
    "    cost_class = 1.0\n",
    "    cost_point = 1.0\n",
    "    focal_alpha = 0.25\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Build model\n",
    "model, criterion, postprocessors = build_conditional_detr(args)\n",
    "model = model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 2048, 1, 1], expected input[2, 256, 64, 64] to have 2048 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Post-process the results\u001b[39;00m\n\u001b[0;32m     27\u001b[0m results \u001b[38;5;241m=\u001b[39m postprocessors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoint\u001b[39m\u001b[38;5;124m'\u001b[39m](outputs, torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m synthetic_images]))\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1529\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1536\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1537\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 25\u001b[0m, in \u001b[0;36mDETRsegm.forward\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m     23\u001b[0m src, mask \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdecompose()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m src_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m hs, memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetr\u001b[38;5;241m.\u001b[39mtransformer(src_proj, mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetr\u001b[38;5;241m.\u001b[39mquery_embed\u001b[38;5;241m.\u001b[39mweight, pos[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     28\u001b[0m outputs_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetr\u001b[38;5;241m.\u001b[39mclass_embed(hs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1529\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1536\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1537\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 2048, 1, 1], expected input[2, 256, 64, 64] to have 2048 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "# Convert to nested tensor format\n",
    "def to_nested_tensor(tensors):\n",
    "    if tensors[0].ndim == 3:\n",
    "        max_size = [max(s) for s in zip(*[img.shape for img in tensors])]\n",
    "        batch_shape = [len(tensors)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensors[0].dtype\n",
    "        device = tensors[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "        for img, pad_img, m in zip(tensors, tensor, mask):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "            m[: img.shape[1], :img.shape[2]] = False\n",
    "    else:\n",
    "        raise ValueError('Unsupported tensor list format')\n",
    "    return NestedTensor(tensor, mask)\n",
    "\n",
    "# Preprocess data\n",
    "nested_samples = to_nested_tensor([img.to(args.device) for img in synthetic_images])\n",
    "\n",
    "# Forward pass through the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(nested_samples)\n",
    "\n",
    "# Post-process the results\n",
    "results = postprocessors['point'](outputs, torch.tensor([list(img.shape[1:]) for img in synthetic_images]))\n",
    "\n",
    "if args.masks:\n",
    "    seg_results = postprocessors['segm'](results, outputs, torch.tensor([list(img.shape[1:]) for img in synthetic_images]), torch.tensor([list(img.shape[1:]) for img in synthetic_images]))\n",
    "\n",
    "# Display results\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"Image {i}:\")\n",
    "    print(f\"Scores: {res['scores']}\")\n",
    "    print(f\"Labels: {res['labels']}\")\n",
    "    print(f\"Points: {res['points']}\")\n",
    "\n",
    "    if args.masks:\n",
    "        print(f\"Masks: {seg_results[i]['masks'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
