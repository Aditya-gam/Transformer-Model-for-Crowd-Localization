{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_, constant_, xavier_normal_\n",
    "import torchvision\n",
    "from torchvision.models._utils import IntermediateLayerGetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "- **PositionEmbeddingSine**: It now calculates the sinusoidal embeddings based on the positions of elements in the image grid, suitable for transformers.\n",
    "- **PositionEmbeddingLearned**: Adjusted to initialize weights more robustly and provided parameters for max dimensions to accommodate different image sizes.\n",
    "- **build_position_encoding**: This function now supports creating either 'sine' or 'learned' embeddings based on the input arguments, making it flexible for different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"Sinusoidal Position Embedding for image-based inputs.\"\"\"\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        self.scale = scale if scale is not None else 2 * math.pi\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.mask is not None, \"Mask cannot be None\"\n",
    "        mask = x.mask\n",
    "        not_mask = ~mask\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "    \n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.tensors.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"Learned Position Embedding for image-based inputs.\"\"\"\n",
    "    def __init__(self, num_pos_feats=256, max_height=50, max_width=50):\n",
    "        super().__init__()\n",
    "        self.row_embed = nn.Embedding(max_height, num_pos_feats)\n",
    "        self.col_embed = nn.Embedding(max_width, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight, -0.05, 0.05)\n",
    "        nn.init.uniform_(self.col_embed.weight, -0.05, 0.05)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.mask is not None, \"Mask cannot be None\"\n",
    "        h, w = x.tensors.shape[-2:]\n",
    "        i = torch.arange(w, device=x.tensors.device)\n",
    "        j = torch.arange(h, device=x.tensors.device)\n",
    "        x_emb = self.col_embed(i)\n",
    "        y_emb = self.row_embed(j)\n",
    "        pos = torch.cat([x_emb.unsqueeze(0).repeat(h, 1, 1), y_emb.unsqueeze(1).repeat(1, w, 1)], dim=-1)\n",
    "        pos = pos.permute(2, 0, 1).unsqueeze(0).repeat(x.tensors.size(0), 1, 1, 1)\n",
    "        \n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_position_encoding(hidden_dim, position_embedding='sine', max_height=50, max_width=50):\n",
    "    \"\"\"Factory function to build position encoding.\"\"\"\n",
    "    num_pos_feats = hidden_dim // 2\n",
    "    if position_embedding == 'sine':\n",
    "        return PositionEmbeddingSine(num_pos_feats, normalize=True)\n",
    "    elif position_embedding == 'learned':\n",
    "        return PositionEmbeddingLearned(num_pos_feats, max_height, max_width)\n",
    "    else:\n",
    "        raise ValueError(f\"Not supported position embedding type {position_embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "This refactored code maintains the same overall structure but simplifies the definition and initialization of the backbone and its components. The `FrozenBatchNorm2d` function is streamlined by using a regular `BatchNorm2d` and setting the gradient flags to `False`. The `BackboneBase` is merged and simplified into `SimplifiedBackbone`, and the functionality of handling different returning layers and feature channels is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frozen_batch_norm2d(num_features, eps=1e-5):\n",
    "    \"\"\"Creates a frozen batch normalization layer.\"\"\"\n",
    "    layer = nn.BatchNorm2d(num_features, affine=True)\n",
    "    layer.weight.requires_grad = False\n",
    "    layer.bias.requires_grad = False\n",
    "    layer.running_mean.requires_grad = False\n",
    "    layer.running_var.requires_grad = False\n",
    "    layer.eps = eps\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device):\n",
    "        cast_tensor = self.tensors.to(device)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            assert mask is not None\n",
    "            cast_mask = mask.to(device)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBackbone(nn.Module):\n",
    "    \"\"\"Simplified backbone model for feature extraction.\"\"\"\n",
    "    def __init__(self, model_name, train_layers=False, num_channels=2048, return_interm_layers=False):\n",
    "        super().__init__()\n",
    "        backbone = getattr(torchvision.models, model_name)(pretrained=True, norm_layer=lambda num_features: frozen_batch_norm2d(num_features))\n",
    "\n",
    "        for name, parameter in backbone.named_parameters():\n",
    "            if not train_layers or all(not name.startswith(f'layer{i}') for i in [2, 3, 4]):\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "        return_layers = {f'layer{i}': str(i-1) for i in range(1, 5)} if return_interm_layers else {'layer4': '0'}\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.body(x.tensors)\n",
    "\n",
    "        return {name: NestedTensor(x, x.mask) for name, x in xs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneWithPositionEmbedding(nn.Sequential):\n",
    "    \"\"\"Combines a backbone and a position embedding module.\"\"\"\n",
    "    def __init__(self, backbone, position_embedding):\n",
    "        super().__init__(backbone, position_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self[0](x)\n",
    "        pos = [self[1](x) for x in xs.values()]\n",
    "        return xs, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_refactored_backbone(model_name, train_layers, num_channels, return_interm_layers, hidden_dim, position_embedding_type):\n",
    "    \"\"\"Builds a backbone with integrated position encoding.\"\"\"\n",
    "    position_embedding = build_position_encoding(hidden_dim, position_embedding_type)\n",
    "    backbone = SimplifiedBackbone(model_name, train_layers, num_channels, return_interm_layers)\n",
    "    return BackboneWithPositionEmbedding(backbone, position_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "This version of the Hungarian matcher will compute matching costs using classification scores, point distances, and optionally GIoU scores (if applicable). However, since the original paper discusses a KMO-based matcher, and your model doesn't include explicit GIoU computation for bounding boxes, we'll focus only on classification and point distances for simplicity.\n",
    "\n",
    "\n",
    "1. **Logits to Probabilities**: The matcher uses `softmax` to convert the logits to probabilities. This is crucial for calculating the classification cost as negative log likelihood, which is more stable and interpretable.\n",
    "2. **Cost Calculation**: The matcher combines classification and point costs linearly using specified weights. It supports batch processing where each element in the batch has its own set of targets and predictions.\n",
    "3. **Linear Sum Assignment**: This uses the `linear_sum_assignment` from `SciPy`, which directly finds the minimum cost matching. It's applied batch-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"Computes an assignment between predictions and ground truth targets.\"\"\"\n",
    "    def __init__(self, cost_class: float = 1.0, cost_point: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_point = cost_point\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            outputs: Dict containing at least 'pred_logits' and 'pred_points'.\n",
    "                - 'pred_logits': Tensor of shape [batch_size, num_queries, num_classes]\n",
    "                - 'pred_points': Tensor of shape [batch_size, num_queries, 4] (for bounding box points)\n",
    "            targets: List of dictionaries containing:\n",
    "                - 'labels': Tensor of shape [num_target_points]\n",
    "                - 'points': Tensor of shape [num_target_points, 4]\n",
    "\n",
    "        Returns:\n",
    "            List of tuples for each batch element, containing:\n",
    "            - Index of selected predictions.\n",
    "            - Index of corresponding selected targets.\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs['pred_logits'].shape[:2]\n",
    "        out_prob = outputs['pred_logits'].softmax(-1)  # Convert logits to probabilities\n",
    "        out_points = outputs['pred_points'].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Concatenate all targets across the batch\n",
    "        tgt_points = torch.cat([t['points'] for t in targets]).to(out_points.device)\n",
    "        tgt_labels = torch.cat([t['labels'] for t in targets])\n",
    "\n",
    "        # Compute classification cost using negative log likelihood\n",
    "        cost_class = -out_prob[:, tgt_labels].flatten(0, 1)\n",
    "\n",
    "        # Compute L1 cost between predicted points and target points\n",
    "        cost_point = torch.cdist(out_points, tgt_points, p=1)\n",
    "\n",
    "        # Combine costs\n",
    "        C = self.cost_class * cost_class + self.cost_point * cost_point\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        # Compute assignment for each batch element\n",
    "        indices = [linear_sum_assignment(c) for c in C]\n",
    "        return [(torch.as_tensor(i, dtype=torch.long), torch.as_tensor(j, dtype=torch.long)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matcher(cost_class=1.0, cost_point=1.0):\n",
    "    return HungarianMatcher(cost_class, cost_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "This `MultiheadAttention`:\n",
    "- Directly uses the input dimensions of query, key, and value without additional projection, simplifying the architecture.\n",
    "- Uses scaled dot-product attention mechanism, following the principle outlined in \"Attention is All You Need\".\n",
    "- Provides dropout for regularization and a final linear projection to match the output dimensions to the input dimensions.\n",
    "- Is flexible with regard to whether it returns attention weights, allowing for easier debugging or further manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"Custom implementation of MultiheadAttention to support different dimensions\n",
    "    for query, key, and value without separate projection matrices for each.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameter initialization\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        xavier_uniform_(self.out_proj.weight)\n",
    "        if self.out_proj.bias is not None:\n",
    "            constant_(self.out_proj.bias, 0)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None):\n",
    "        \"\"\" Forward pass for custom multihead attention.\n",
    "        Assumes L = target sequence length, S = source sequence length, N = batch size, E = embedding dimension.\n",
    "        \"\"\"\n",
    "        tgt_len, bsz, embed_dim = query.size()\n",
    "        assert query.size() == key.size() == value.size(), \"Query, Key and Value must be of the same size\"\n",
    "\n",
    "        # Scale query for dot product attention\n",
    "        scaling = self.head_dim ** -0.5\n",
    "        query = query * scaling\n",
    "\n",
    "        # Calculate Q, K, V\n",
    "        q = query.reshape(tgt_len, bsz * self.num_heads, self.head_dim)\n",
    "        k = key.reshape(-1, bsz * self.num_heads, self.head_dim)\n",
    "        v = value.reshape(-1, bsz * self.num_heads, self.head_dim)\n",
    "\n",
    "        # Dot product of Q and K (transpose)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, -1)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf')\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * self.num_heads, tgt_len, -1)\n",
    "\n",
    "        # Apply softmax and dropout on attention weights\n",
    "        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "        attn_output_weights = self.dropout_layer(attn_output_weights)\n",
    "\n",
    "        # Multiply weights by V\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "\n",
    "        # Transpose and reshape to bring back to original dimensions\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "\n",
    "        # Apply final linear projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        if need_weights:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, -1)\n",
    "            attn_output_weights = attn_output_weights.sum(dim=1) / self.num_heads\n",
    "        else:\n",
    "            attn_output_weights = None\n",
    "\n",
    "        return attn_output, attn_output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    \"\"\"Create N identical layers by deep copying the given module.\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    elif activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(\"Activation function {} is not supported\".format(activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, layer, n_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(layer, n_layers)\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.embed_dim)\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        src = self.norm(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, layer, n_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(layer, n_layers)\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.embed_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        tgt = self.norm(tgt)\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\" Container module hosting the encoder and decoder. \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, pos_embed):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.pos_embed = pos_embed\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        memory = self.encoder(self.pos_embed(self.src_embed(src)), src_mask, src_key_padding_mask)\n",
    "        output = self.decoder(self.pos_embed(self.tgt_embed(tgt)), memory, tgt_mask, None, tgt_key_padding_mask, None)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation='relu'):\n",
    "    \"\"\"Function to build the transformer model.\"\"\"\n",
    "    encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "    encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "    \n",
    "    decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "    decoder = TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "    \n",
    "    src_embed = nn.Embedding(num_embeddings=10, embedding_dim=d_model)  # Placeholder for actual embedding logic\n",
    "    tgt_embed = nn.Embedding(num_embeddings=10, embedding_dim=d_model)  # Placeholder for actual embedding logic\n",
    "    pos_encoder = PositionalEncoding(d_model=d_model)\n",
    "    \n",
    "    return TransformerModel(encoder, decoder, src_embed, tgt_embed, pos_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
