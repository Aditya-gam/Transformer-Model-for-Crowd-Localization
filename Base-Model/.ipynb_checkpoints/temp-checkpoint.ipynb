{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71918cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PanopticAPI is not installed. Proceeding without it.\n",
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import copy\n",
    "import datetime\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import time\n",
    "import warnings\n",
    "import builtins as __builtin__\n",
    "from collections import defaultdict, deque\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import scipy.spatial\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# PyTorch related imports\n",
    "import torch\n",
    "from torch import nn, Tensor, _VF, jit\n",
    "import torch.distributed as dist\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import linear, pad, softmax, dropout\n",
    "from torch._jit_internal import Optional, Tuple\n",
    "\n",
    "# Torchvision related imports\n",
    "import torchvision\n",
    "from torchvision.ops.boxes import box_area\n",
    "from torchvision.models._utils import IntermediateLayerGetter  \n",
    "if float(torchvision.__version__.split(\".\")[1]) < 7.0:\n",
    "    from torchvision.ops import _new_empty_tensor\n",
    "    from torchvision.ops.misc import _output_size\n",
    "\n",
    "# Conditional imports based on PyTorch version\n",
    "if int(torch.__version__[0:-2].split('.')[0])*100 + int(torch.__version__[0:-2].split('.')[1]) < 107:\n",
    "    from torch._overrides import has_torch_function, handle_torch_function\n",
    "    from torch.nn.modules.linear import _LinearWithBias\n",
    "else:\n",
    "    from torch.overrides import has_torch_function, handle_torch_function\n",
    "    from torch.nn.modules.linear import Linear as _LinearWithBias\n",
    "\n",
    "# Optional imports for external utilities\n",
    "try:\n",
    "    from panopticapi.utils import id2rgb, rgb2id\n",
    "except ImportError:\n",
    "    print(\"PanopticAPI is not installed. Proceeding without it.\")\n",
    "\n",
    "# Global device setting\n",
    "# Tensor = torch.Tensor \n",
    "Device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using Device: {Device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333a75d",
   "metadata": {},
   "source": [
    "# Util Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64136973",
   "metadata": {},
   "source": [
    "## box_ops file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00925ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(boxes):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from (center x, center y, width, height) to (xmin, ymin, xmax, ymax) format.\n",
    "    \n",
    "    Args:\n",
    "        boxes (Tensor): The bounding boxes in (cx, cy, w, h) format.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: The bounding boxes in (xmin, ymin, xmax, ymax) format.\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    half_w, half_h = 0.5 * w, 0.5 * h\n",
    "    return torch.stack([cx - half_w, cy - half_h, cx + half_w, cy + half_h], dim=-1)\n",
    "\n",
    "def box_xyxy_to_cxcywh(boxes):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from (xmin, ymin, xmax, ymax) to (center x, center y, width, height) format.\n",
    "    \n",
    "    Args:\n",
    "        boxes (Tensor): The bounding boxes in (xmin, ymin, xmax, ymax) format.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: The bounding boxes in (cx, cy, w, h) format.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = boxes.unbind(-1)\n",
    "    width, height = x_max - x_min, y_max - y_min\n",
    "    center_x, center_y = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "    return torch.stack([center_x, center_y, width, height], dim=-1)\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two sets of boxes.\n",
    "    \n",
    "    Args:\n",
    "        boxes1 (Tensor): First set of boxes in (xmin, ymin, xmax, ymax) format.\n",
    "        boxes2 (Tensor): Second set of boxes in the same format.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: The IoU values.\n",
    "    \"\"\"\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "\n",
    "    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    wh = (right_bottom - left_top).clamp(min=0)\n",
    "    intersection = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    union = area1[:, None] + area2 - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39285247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Calculate the Generalized Intersection over Union (GIoU) for two sets of boxes.\n",
    "    GIoU is an improvement over the traditional IoU metric for bounding box regression, helping in cases\n",
    "    where the IoU is zero but the boxes are near each other.\n",
    "    \n",
    "    Args:\n",
    "        boxes1 (Tensor): Bounding boxes tensor with shape [N, 4] in [x0, y0, x1, y1] format.\n",
    "        boxes2 (Tensor): Bounding boxes tensor with shape [M, 4] in [x0, y0, x1, y1] format.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: Tensor of shape [N, M] containing the GIoUs for all pairs of boxes.\n",
    "    \"\"\"\n",
    "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all(), \"Box tensor boxes1 is malformed.\"\n",
    "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all(), \"Box tensor boxes2 is malformed.\"\n",
    "    \n",
    "    iou, union = box_iou(boxes1, boxes2)\n",
    "\n",
    "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    enclosing_area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    return iou - (enclosing_area - union) / enclosing_area\n",
    "\n",
    "\n",
    "def masks_to_boxes(masks):\n",
    "    \"\"\"\n",
    "    Derive bounding boxes from masks.\n",
    "    \n",
    "    Args:\n",
    "        masks (Tensor): Binary masks of shape [N, H, W].\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: A tensor of bounding boxes [N, 4] with each box encoded as [x_min, y_min, x_max, y_max].\n",
    "    \"\"\"\n",
    "    if masks.numel() == 0:\n",
    "        return torch.zeros((0, 4), device=masks.device)\n",
    "    \n",
    "    n, h, w = masks.shape\n",
    "    y_coords, x_coords = torch.meshgrid(torch.arange(h, device=masks.device), \n",
    "                                        torch.arange(w, device=masks.device), indexing='ij')\n",
    "\n",
    "    x_mask = masks * x_coords[None, :, :]\n",
    "    x_max = x_mask.max(dim=2).values\n",
    "    x_min = x_mask.masked_fill(~masks, w).min(dim=2).values\n",
    "\n",
    "    y_mask = masks * y_coords[None, :, :]\n",
    "    y_max = y_mask.max(dim=2).values\n",
    "    y_min = y_mask.masked_fill(~masks, h).min(dim=2).values\n",
    "\n",
    "    return torch.stack([x_min, y_min, x_max, y_max], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14376f",
   "metadata": {},
   "source": [
    "## misc File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582abf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedUtils:\n",
    "    \"\"\"\n",
    "    A utility class to encapsulate distributed computing functionalities,\n",
    "    including custom print redirection and checks for distributed process statuses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.original_print = __builtin__.print\n",
    "\n",
    "    def setup_for_distributed(self, is_master):\n",
    "        \"\"\"\n",
    "        Redirects the built-in print function to only output if this process is the master.\n",
    "        \"\"\"\n",
    "        def print(*args, **kwargs):\n",
    "            force = kwargs.pop('force', False)\n",
    "            if is_master or force:\n",
    "                self.original_print(*args, **kwargs)\n",
    "        __builtin__.print = print\n",
    "\n",
    "    def is_distributed_available_and_initialized(self):\n",
    "        \"\"\"\n",
    "        Check if the distributed processing is available and initialized.\n",
    "        \"\"\"\n",
    "        return dist.is_available() and dist.is_initialized()\n",
    "\n",
    "    def get_world_size(self):\n",
    "        \"\"\"\n",
    "        Get the number of processes in the current group.\n",
    "        \"\"\"\n",
    "        if not self.is_distributed_available_and_initialized():\n",
    "            return 1\n",
    "        return dist.get_world_size()\n",
    "\n",
    "    def get_rank(self):\n",
    "        \"\"\"\n",
    "        Get the rank of the current process in the distributed group.\n",
    "        \"\"\"\n",
    "        if not self.is_distributed_available_and_initialized():\n",
    "            return 0\n",
    "        return dist.get_rank()\n",
    "\n",
    "    def is_main_process(self):\n",
    "        \"\"\"\n",
    "        Check if the current process is the main (or master) process.\n",
    "        \"\"\"\n",
    "        return self.get_rank() == 0\n",
    "\n",
    "    def save_on_master(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Saves a checkpoint only from the master process.\n",
    "        \"\"\"\n",
    "        if self.is_main_process():\n",
    "            torch.save(*args, **kwargs)\n",
    "\n",
    "dist_utils = DistributedUtils()\n",
    "dist_utils.setup_for_distributed(is_master=dist_utils.is_main_process())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a33a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue:\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size=20, fmt=\"{median:.4f} ({global_avg:.4f})\"):\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "        self._cache_dirty = True\n",
    "        self._cached_median = None\n",
    "        self._cached_avg = None\n",
    "        self._cached_max = None\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.total += value * n\n",
    "        self.count += n\n",
    "        self._cache_dirty = True\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        if not (dist.is_available() and dist.is_initialized()):\n",
    "            return\n",
    "        tensor = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(tensor)\n",
    "        self.count, self.total = int(tensor[0].item()), tensor[1].item()\n",
    "    \n",
    "    @property\n",
    "    def median(self):\n",
    "        if self._cache_dirty:\n",
    "            self._cached_median = torch.tensor(list(self.deque)).median().item()\n",
    "        return self._cached_median\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        if self._cache_dirty:\n",
    "            self._cached_avg = torch.tensor(list(self.deque), dtype=torch.float32).mean().item()\n",
    "        return self._cached_avg\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count if self.count else 0\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        if self._cache_dirty:\n",
    "            self._cached_max = max(self.deque)\n",
    "        return self._cached_max\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1] if self.deque else None\n",
    "\n",
    "    def __str__(self):\n",
    "        if self._cache_dirty:\n",
    "            # Only update the cached values when necessary\n",
    "            self.median()\n",
    "            self.avg()\n",
    "            self.max()\n",
    "            self._cache_dirty = False\n",
    "        return self.fmt.format(\n",
    "            median=self._cached_median,\n",
    "            avg=self._cached_avg,\n",
    "            global_avg=self.global_avg(),\n",
    "            max=self._cached_max,\n",
    "            value=self.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f4a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_gather(data):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    world_size = dist_utils.get_world_size()\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    # serialized to a Tensor\n",
    "    buffer = pickle.dumps(data)\n",
    "    storage = torch.ByteStorage.from_buffer(buffer)\n",
    "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
    "\n",
    "    # obtain Tensor size of each rank\n",
    "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
    "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
    "    dist.all_gather(size_list, local_size)\n",
    "    size_list = [int(size.item()) for size in size_list]\n",
    "    max_size = max(size_list)\n",
    "\n",
    "    # receiving Tensor from all ranks\n",
    "    # we pad the tensor because torch all_gather does not support\n",
    "    # gathering tensors of different shapes\n",
    "    tensor_list = []\n",
    "    for _ in size_list:\n",
    "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
    "    if local_size != max_size:\n",
    "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    dist.all_gather(tensor_list, tensor)\n",
    "\n",
    "    data_list = []\n",
    "    for size, tensor in zip(size_list, tensor_list):\n",
    "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
    "        data_list.append(pickle.loads(buffer))\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a020848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_dictionary(data_dict, use_average=True):\n",
    "    \"\"\"\n",
    "    Aggregate dictionary values across all processes in a distributed computing environment.\n",
    "    This function reduces the dictionary values to either their sum or average.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): Dictionary with tensor values that will be reduced across processes.\n",
    "        use_average (bool): If True, averages the values, otherwise sums them.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the same keys as `data_dict`, containing reduced (summed or averaged) values.\n",
    "    \"\"\"\n",
    "    total_processes = dist_utils.get_world_size()\n",
    "    if total_processes < 2:\n",
    "        return data_dict\n",
    "\n",
    "    with torch.no_grad():\n",
    "        keys = []\n",
    "        tensors = []\n",
    "        # Ensure keys are sorted to maintain consistency across processes.\n",
    "        for key in sorted(data_dict.keys()):\n",
    "            keys.append(key)\n",
    "            tensors.append(data_dict[key])\n",
    "        tensors = torch.stack(tensors, dim=0)\n",
    "        dist.all_reduce(tensors)\n",
    "        if use_average:\n",
    "            tensors /= total_processes\n",
    "        aggregated_dict = {key: tensor for key, tensor in zip(keys, tensors)}\n",
    "    return aggregated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b32218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fdfe47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device):\n",
    "        cast_tensor = self.tensors.to(device)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            assert mask is not None\n",
    "            cast_mask = mask.to(device)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58c05cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _onnx_nested_tensor_from_tensor_list() is an implementation of\n",
    "# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n",
    "@torch.jit.unused\n",
    "def _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n",
    "    max_size = []\n",
    "    for i in range(tensor_list[0].dim()):\n",
    "        max_size_i = torch.max(torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)).to(torch.int64)\n",
    "        max_size.append(max_size_i)\n",
    "    max_size = tuple(max_size)\n",
    "\n",
    "    # work around for\n",
    "    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "    # m[: img.shape[1], :img.shape[2]] = False\n",
    "    # which is not yet supported in onnx\n",
    "    padded_imgs = []\n",
    "    padded_masks = []\n",
    "    for img in tensor_list:\n",
    "        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
    "        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
    "        padded_imgs.append(padded_img)\n",
    "\n",
    "        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n",
    "        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n",
    "        padded_masks.append(padded_mask.to(torch.bool))\n",
    "\n",
    "    tensor = torch.stack(padded_imgs)\n",
    "    mask = torch.stack(padded_masks)\n",
    "\n",
    "    return NestedTensor(tensor, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c749aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _max_by_axis(the_list):\n",
    "    # type: (List[List[int]]) -> List[int]\n",
    "    maxes = the_list[0]\n",
    "    for sublist in the_list[1:]:\n",
    "        for index, item in enumerate(sublist):\n",
    "            maxes[index] = max(maxes[index], item)\n",
    "    return maxes\n",
    "\n",
    "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "    # make this more general\n",
    "    if tensor_list[0].ndim == 3:\n",
    "        if torchvision._is_tracing():\n",
    "            # nested_tensor_from_tensor_list() does not export well to ONNX\n",
    "            # call _onnx_nested_tensor_from_tensor_list() instead\n",
    "            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n",
    "\n",
    "        # make it support different-sized images\n",
    "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "            m[: img.shape[1], :img.shape[2]] = False\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return NestedTensor(tensor, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63947b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sha():\n",
    "    cwd = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "    def _run(command):\n",
    "        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n",
    "\n",
    "    sha = 'N/A'\n",
    "    diff = \"clean\"\n",
    "    branch = 'N/A'\n",
    "    try:\n",
    "        sha = _run(['git', 'rev-parse', 'HEAD'])\n",
    "        subprocess.check_output(['git', 'diff'], cwd=cwd)\n",
    "        diff = _run(['git', 'diff-index', 'HEAD'])\n",
    "        diff = \"has uncommited changes\" if diff else \"clean\"\n",
    "        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n",
    "    except Exception:\n",
    "        pass\n",
    "    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n",
    "    return message\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
    "    return tuple(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec8a9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_distributed_mode(args):\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}'.format(\n",
    "        args.rank, args.dist_url), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    dist_utils.setup_for_distributed(args.rank == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "838f466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    if target.numel() == 0:\n",
    "        return [torch.zeros([], device=output.device)]\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
    "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
    "    \"\"\"\n",
    "    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n",
    "    This will eventually be supported natively by PyTorch, and this\n",
    "    class can go away.\n",
    "    \"\"\"\n",
    "    if float(torchvision.__version__.split(\".\")[1]) < 7.0:\n",
    "        if input.numel() > 0:\n",
    "            return torch.nn.functional.interpolate(\n",
    "                input, size, scale_factor, mode, align_corners\n",
    "            )\n",
    "\n",
    "        output_shape = _output_size(2, input, size, scale_factor)\n",
    "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
    "        return _new_empty_tensor(input, output_shape)\n",
    "    else:\n",
    "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)\n",
    "\n",
    "\n",
    "def inverse_sigmoid(x, eps=1e-5):\n",
    "    x = x.clamp(min=0, max=1)\n",
    "    x1 = x.clamp(min=eps)\n",
    "    x2 = (1 - x).clamp(min=eps)\n",
    "    return torch.log(x1 / x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662fe43",
   "metadata": {},
   "source": [
    "# Networks/CDETR Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbf226",
   "metadata": {},
   "source": [
    "## attention File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6338ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_forward(query: Tensor, key: Tensor, value: Tensor, embed_dim_to_check: int, num_heads: int, in_proj_weight: Tensor, in_proj_bias: Tensor, bias_k: Optional[Tensor], bias_v: Optional[Tensor], add_zero_attn: bool, dropout_p: float, out_proj_weight: Tensor, out_proj_bias: Tensor, training: bool = True, key_padding_mask: Optional[Tensor] = None, need_weights: bool = True, attn_mask: Optional[Tensor] = None, use_separate_proj_weight: bool = False, q_proj_weight: Optional[Tensor] = None, k_proj_weight: Optional[Tensor] = None, v_proj_weight: Optional[Tensor] = None, static_k: Optional[Tensor] = None, static_v: Optional[Tensor] = None, out_dim: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
    "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting():\n",
    "        tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v,\n",
    "                    out_proj_weight, out_proj_bias)\n",
    "        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):\n",
    "            return handle_torch_function(\n",
    "                multi_head_attention_forward, tens_ops, query, key, value,\n",
    "                embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias,\n",
    "                bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight,\n",
    "                out_proj_bias, training=training, key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights, attn_mask=attn_mask,\n",
    "                use_separate_proj_weight=use_separate_proj_weight,\n",
    "                q_proj_weight=q_proj_weight, k_proj_weight=k_proj_weight,\n",
    "                v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v)\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    # allow MHA to have different sizes for the feature dimension\n",
    "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    v_head_dim = out_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    q = query * scaling\n",
    "    k = key\n",
    "    v = value\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n",
    "            attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool, \\\n",
    "            'Only float, byte, and bool types are supported for attn_mask, not {}'.format(\n",
    "                attn_mask.dtype)\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\n",
    "                \"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError(\n",
    "                    'The size of the 2D attn_mask is not correct.')\n",
    "        elif attn_mask.dim() == 3:\n",
    "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError(\n",
    "                    'The size of the 3D attn_mask is not correct.')\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
    "        # attn_mask's dim is 3 now.\n",
    "\n",
    "    # convert ByteTensor key_padding_mask to bool\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\n",
    "            \"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, v_head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == v_head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()\n",
    "                      [2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()\n",
    "                      [2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    assert list(attn_output_weights.size()) == [\n",
    "        bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "        else:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(\n",
    "            bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(\n",
    "            bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    attn_output_weights = softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "    attn_output_weights = dropout(\n",
    "        attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, v_head_dim]\n",
    "    attn_output = attn_output.transpose(\n",
    "        0, 1).contiguous().view(tgt_len, bsz, out_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(\n",
    "            bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ac46cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
    "        query, key, and value have the same number of features.\n",
    "    Examples::\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "    \"\"\"\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None,\n",
    "                 vdim=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.out_proj = _LinearWithBias(vdim, vdim)\n",
    "\n",
    "        self.in_proj_bias = None\n",
    "        self.in_proj_weight = None\n",
    "        self.bias_k = self.bias_v = None\n",
    "        self.q_proj_weight = None\n",
    "        self.k_proj_weight = None\n",
    "        self.v_proj_weight = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super(MultiheadAttention, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored. When given\n",
    "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
    "            layer will be ignored\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
    "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*\\text{num_heads}, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "        \"\"\"\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight, out_dim=self.vdim)\n",
    "        else:\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, out_dim=self.vdim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07555bbb",
   "metadata": {},
   "source": [
    "## matcher File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94cff386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, cost_class: float = 1, cost_point: float = 1, cost_giou: float = 1):\n",
    "        \"\"\"Creates the matcher\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_point: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
    "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_point = cost_point\n",
    "        self.args = args\n",
    "        assert cost_class != 0 or cost_point != 0, \"all costs cant be 0\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"pred_points\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_points] (where num_target_points is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"points\": Tensor of dim [num_target_points, 4] containing the target box coordinates\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_points)\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).sigmoid()  # [batch_size * num_queries, num_classes]\n",
    "\n",
    "\n",
    "        out_point = outputs[\"pred_points\"].flatten(0, 1)\n",
    "        tgt_point = torch.cat([v[\"points\"] for v in targets])\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost.\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "        pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "        cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "\n",
    "\n",
    "\n",
    "        cost_point = torch.cdist(out_point, tgt_point.cuda(), p=1)\n",
    "\n",
    "        C = self.cost_class * cost_class + self.cost_point * cost_point\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"points\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bac8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matcher(args):\n",
    "    return HungarianMatcher(args, cost_class=args.set_cost_class, cost_point=args.set_cost_point, cost_giou=args.set_cost_giou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88271e97",
   "metadata": {},
   "source": [
    "## transformer file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ace6f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f01eeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ab7d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sineembed_for_position(pos_tensor):\n",
    "    scale = 2 * math.pi\n",
    "    dim_t = torch.arange(128, dtype=torch.float32, device=pos_tensor.device)\n",
    "    dim_t = 10000 ** (2 * (dim_t // 2) / 128)\n",
    "    x_embed = pos_tensor[:, :, 0] * scale\n",
    "    y_embed = pos_tensor[:, :, 1] * scale\n",
    "    pos_x = x_embed[:, :, None] / dim_t\n",
    "    pos_y = y_embed[:, :, None] / dim_t\n",
    "    pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    pos_y = torch.stack((pos_y[:, :, 0::2].sin(), pos_y[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    pos = torch.cat((pos_y, pos_x), dim=2)\n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0682699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self,\n",
    "                     src,\n",
    "                     src_mask: Optional[Tensor] = None,\n",
    "                     src_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(src, pos)\n",
    "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "    def forward_pre(self, src,\n",
    "                    src_mask: Optional[Tensor] = None,\n",
    "                    src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None):\n",
    "        src2 = self.norm1(src)\n",
    "        q = k = self.with_pos_embed(src2, pos)\n",
    "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        return src\n",
    "\n",
    "    def forward(self, src,\n",
    "                src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
    "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec1d467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        # Decoder Self-Attention\n",
    "        self.sa_qcontent_proj = nn.Linear(d_model, d_model)\n",
    "        self.sa_qpos_proj = nn.Linear(d_model, d_model)\n",
    "        self.sa_kcontent_proj = nn.Linear(d_model, d_model)\n",
    "        self.sa_kpos_proj = nn.Linear(d_model, d_model)\n",
    "        self.sa_v_proj = nn.Linear(d_model, d_model)\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, vdim=d_model)\n",
    "\n",
    "        # Decoder Cross-Attention\n",
    "        self.ca_qcontent_proj = nn.Linear(d_model, d_model)\n",
    "        self.ca_qpos_proj = nn.Linear(d_model, d_model)\n",
    "        self.ca_kcontent_proj = nn.Linear(d_model, d_model)\n",
    "        self.ca_kpos_proj = nn.Linear(d_model, d_model)\n",
    "        self.ca_v_proj = nn.Linear(d_model, d_model)\n",
    "        self.ca_qpos_sine_proj = nn.Linear(d_model, d_model)\n",
    "        self.cross_attn = MultiheadAttention(d_model * 2, nhead, dropout=dropout, vdim=d_model)\n",
    "\n",
    "        self.nhead = nhead\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self, tgt, memory,\n",
    "                     tgt_mask: Optional[Tensor] = None,\n",
    "                     memory_mask: Optional[Tensor] = None,\n",
    "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None,\n",
    "                     query_pos: Optional[Tensor] = None,\n",
    "                     query_sine_embed=None,\n",
    "                     is_first=False):\n",
    "\n",
    "        # ========== Begin of Self-Attention =============\n",
    "        # Apply projections here\n",
    "        # shape: num_queries x batch_size x 256\n",
    "        q_content = self.sa_qcontent_proj(tgt)  # target is the input of the first decoder layer. zero by default.\n",
    "        q_pos = self.sa_qpos_proj(query_pos)\n",
    "        k_content = self.sa_kcontent_proj(tgt)\n",
    "        k_pos = self.sa_kpos_proj(query_pos)\n",
    "        v = self.sa_v_proj(tgt)\n",
    "\n",
    "        num_queries, bs, n_model = q_content.shape\n",
    "        hw, _, _ = k_content.shape\n",
    "\n",
    "        q = q_content + q_pos\n",
    "        k = k_content + k_pos\n",
    "\n",
    "        tgt2 = self.self_attn(q, k, value=v, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        # ========== End of Self-Attention =============\n",
    "\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # ========== Begin of Cross-Attention =============\n",
    "        # Apply projections here\n",
    "        # shape: num_queries x batch_size x 256\n",
    "        q_content = self.ca_qcontent_proj(tgt)\n",
    "        k_content = self.ca_kcontent_proj(memory)\n",
    "        v = self.ca_v_proj(memory)\n",
    "\n",
    "        num_queries, bs, n_model = q_content.shape\n",
    "        hw, _, _ = k_content.shape\n",
    "\n",
    "        k_pos = self.ca_kpos_proj(pos)\n",
    "\n",
    "        # For the first decoder layer, we concatenate the positional embedding predicted from \n",
    "        # the object query (the positional embedding) into the original query (key) in DETR.\n",
    "        if is_first:\n",
    "            q_pos = self.ca_qpos_proj(query_pos)\n",
    "            q = q_content + q_pos\n",
    "            k = k_content + k_pos\n",
    "        else:\n",
    "            q = q_content\n",
    "            k = k_content\n",
    "\n",
    "        q = q.view(num_queries, bs, self.nhead, n_model // self.nhead)\n",
    "        query_sine_embed = self.ca_qpos_sine_proj(query_sine_embed)\n",
    "        query_sine_embed = query_sine_embed.view(num_queries, bs, self.nhead, n_model // self.nhead)\n",
    "        q = torch.cat([q, query_sine_embed], dim=3).view(num_queries, bs, n_model * 2)\n",
    "        k = k.view(hw, bs, self.nhead, n_model // self.nhead)\n",
    "        k_pos = k_pos.view(hw, bs, self.nhead, n_model // self.nhead)\n",
    "        k = torch.cat([k, k_pos], dim=3).view(hw, bs, n_model * 2)\n",
    "\n",
    "        tgt2 = self.cross_attn(query=q,\n",
    "                               key=k,\n",
    "                               value=v, attn_mask=memory_mask,\n",
    "                               key_padding_mask=memory_key_padding_mask)[0]\n",
    "        # ========== End of Cross-Attention =============\n",
    "\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward_pre(self, tgt, memory,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None,\n",
    "                    query_pos: Optional[Tensor] = None):\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
    "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask: Optional[Tensor] = None, \n",
    "            memory_mask: Optional[Tensor] = None, \n",
    "            tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "            memory_key_padding_mask: Optional[Tensor] = None, \n",
    "            pos: Optional[Tensor] = None, \n",
    "            query_pos: Optional[Tensor] = None, \n",
    "            query_sine_embed=None, \n",
    "            is_first=False):\n",
    "    \n",
    "        if self.normalize_before:\n",
    "            # Handle the case where normalization is required before processing\n",
    "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
    "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
    "        else:\n",
    "            # Handle the case where normalization is not required before processing\n",
    "            return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
    "                                    tgt_key_padding_mask, memory_key_padding_mask, \n",
    "                                    pos, query_pos, query_sine_embed, is_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d3322ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src,\n",
    "                mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        output = src\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=mask,\n",
    "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8541668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False, d_model=256):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "        self.query_scale = MLP(d_model, d_model, d_model, 2)\n",
    "        self.ref_point_head = MLP(d_model, d_model, 2, 2)\n",
    "        for layer_id in range(num_layers - 1):\n",
    "            self.layers[layer_id + 1].ca_qpos_proj = None\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "        reference_points_before_sigmoid = self.ref_point_head(query_pos)  # [num_queries, batch_size, 2]\n",
    "        reference_points = reference_points_before_sigmoid.sigmoid().transpose(0, 1)\n",
    "\n",
    "        for layer_id, layer in enumerate(self.layers):\n",
    "            obj_center = reference_points[..., :2].transpose(0, 1)  # [num_queries, batch_size, 2]\n",
    "\n",
    "            # For the first decoder layer, we do not apply transformation over p_s\n",
    "            if layer_id == 0:\n",
    "                pos_transformation = 1\n",
    "            else:\n",
    "                pos_transformation = self.query_scale(output)\n",
    "\n",
    "            # get sine embedding for the query vector\n",
    "            query_sine_embed = gen_sineembed_for_position(obj_center)\n",
    "            # apply transformation\n",
    "            query_sine_embed = query_sine_embed * pos_transformation\n",
    "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
    "                           memory_mask=memory_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                           memory_key_padding_mask=memory_key_padding_mask,\n",
    "                           pos=pos, query_pos=query_pos, query_sine_embed=query_sine_embed,\n",
    "                           is_first=(layer_id == 0))\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(self.norm(output))\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.pop()\n",
    "                intermediate.append(output)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return [torch.stack(intermediate).transpose(1, 2), reference_points]\n",
    "\n",
    "        return output.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be1013ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=512, nhead=8, num_queries=300, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False,\n",
    "                 return_intermediate_dec=False):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
    "                                          return_intermediate=return_intermediate_dec,\n",
    "                                          d_model=d_model)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dec_layers = num_decoder_layers\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, mask, query_embed, pos_embed):\n",
    "        # flatten NxCxHxW to HWxNxC\n",
    "        bs, c, h, w = src.shape\n",
    "        src = src.flatten(2).permute(2, 0, 1)\n",
    "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
    "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
    "        mask = mask.flatten(1)\n",
    "\n",
    "        tgt = torch.zeros_like(query_embed)\n",
    "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
    "        hs, references = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n",
    "                                      pos=pos_embed, query_pos=query_embed)\n",
    "        return hs, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc07b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(args):\n",
    "    return Transformer(\n",
    "        d_model=args.hidden_dim,\n",
    "        dropout=args.dropout,\n",
    "        nhead=args.nheads,\n",
    "        num_queries=args.num_queries,\n",
    "        dim_feedforward=args.dim_feedforward,\n",
    "        num_encoder_layers=args.enc_layers,\n",
    "        num_decoder_layers=args.dec_layers,\n",
    "        normalize_before=args.pre_norm,\n",
    "        return_intermediate_dec=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a8704",
   "metadata": {},
   "source": [
    "## positional_encoding File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "190ee90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a more standard version of the position embedding, very similar to the one\n",
    "    used by the Attention is all you need paper, generalized to work on images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        x = tensor_list.tensors\n",
    "        mask = tensor_list.mask\n",
    "        assert mask is not None\n",
    "        not_mask = ~mask\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49ea8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"\n",
    "    Absolute pos embedding, learned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats=256):\n",
    "        super().__init__()\n",
    "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight)\n",
    "        nn.init.uniform_(self.col_embed.weight)\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        x = tensor_list.tensors\n",
    "        h, w = x.shape[-2:]\n",
    "        i = torch.arange(w, device=x.device)\n",
    "        j = torch.arange(h, device=x.device)\n",
    "        x_emb = self.col_embed(i)\n",
    "        y_emb = self.row_embed(j)\n",
    "        pos = torch.cat([\n",
    "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
    "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
    "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93fd1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_position_encoding(args):\n",
    "    N_steps = args.hidden_dim // 2\n",
    "    if args.position_embedding in ('v2', 'sine'):\n",
    "        # find a better way of exposing other arguments\n",
    "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
    "    elif args.position_embedding in ('v3', 'learned'):\n",
    "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
    "    else:\n",
    "        raise ValueError(f\"not supported {args.position_embedding}\")\n",
    "\n",
    "    return position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480a690",
   "metadata": {},
   "source": [
    "## backbone File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83640fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
    "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
    "    produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = 1e-5\n",
    "        scale = w * (rv + eps).rsqrt()\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1532a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneBase(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n",
    "        super().__init__()\n",
    "        for name, parameter in backbone.named_parameters():\n",
    "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
    "                parameter.requires_grad_(False)\n",
    "        if return_interm_layers:\n",
    "            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
    "        else:\n",
    "            return_layers = {'layer4': \"0\"}\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        xs = self.body(tensor_list.tensors)\n",
    "        out: Dict[str, NestedTensor] = {}\n",
    "        for name, x in xs.items():\n",
    "            m = tensor_list.mask\n",
    "            assert m is not None\n",
    "            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
    "            out[name] = NestedTensor(x, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c12aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(BackboneBase):\n",
    "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str,\n",
    "                 train_backbone: bool,\n",
    "                 return_interm_layers: bool,\n",
    "                 dilation: bool):\n",
    "        backbone = getattr(torchvision.models, name)(\n",
    "            replace_stride_with_dilation=[False, False, dilation],\n",
    "            pretrained=True, norm_layer=FrozenBatchNorm2d)\n",
    "        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
    "        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7556516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joiner(nn.Sequential):\n",
    "    def __init__(self, backbone, position_embedding):\n",
    "        super().__init__(backbone, position_embedding)\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        xs = self[0](tensor_list)\n",
    "        out: List[NestedTensor] = []\n",
    "        pos = []\n",
    "        for name, x in xs.items():\n",
    "            out.append(x)\n",
    "            # position encoding\n",
    "            pos.append(self[1](x).to(x.tensors.dtype))\n",
    "\n",
    "        return out, pos\n",
    "\n",
    "\n",
    "def build_backbone(args):\n",
    "    position_embedding = build_position_encoding(args)\n",
    "    train_backbone = args.lr_backbone > 0\n",
    "    return_interm_layers = args.masks\n",
    "    backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)\n",
    "    model = Joiner(backbone, position_embedding)\n",
    "    model.num_channels = backbone.num_channels\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a79075",
   "metadata": {},
   "source": [
    "## segmentation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46efbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand(tensor, length: int):\n",
    "    return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "931d6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskHeadSmallConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional head, using group norm.\n",
    "    Upsampling is done using a FPN approach\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, fpn_dims, context_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n",
    "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = torch.nn.GroupNorm(8, dim)\n",
    "        self.lay2 = torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
    "        self.gn2 = torch.nn.GroupNorm(8, inter_dims[1])\n",
    "        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
    "        self.gn3 = torch.nn.GroupNorm(8, inter_dims[2])\n",
    "        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
    "        self.gn4 = torch.nn.GroupNorm(8, inter_dims[3])\n",
    "        self.lay5 = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
    "        self.gn5 = torch.nn.GroupNorm(8, inter_dims[4])\n",
    "        self.out_lay = torch.nn.Conv2d(inter_dims[4], 1, 3, padding=1)\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
    "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
    "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):\n",
    "        x = torch.cat([_expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)\n",
    "\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.out_lay(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92e62482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAttentionMap(nn.Module):\n",
    "    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n",
    "\n",
    "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "\n",
    "        nn.init.zeros_(self.k_linear.bias)\n",
    "        nn.init.zeros_(self.q_linear.bias)\n",
    "        nn.init.xavier_uniform_(self.k_linear.weight)\n",
    "        nn.init.xavier_uniform_(self.q_linear.weight)\n",
    "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, q, k, mask: Optional[Tensor] = None):\n",
    "        q = self.q_linear(q)\n",
    "        k = F.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
    "        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
    "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", qh * self.normalize_fact, kh)\n",
    "\n",
    "        if mask is not None:\n",
    "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "        weights = F.softmax(weights.flatten(2), dim=-1).view(weights.size())\n",
    "        weights = self.dropout(weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95b60a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRsegm(nn.Module):\n",
    "    def __init__(self, detr, freeze_detr=False):\n",
    "        super().__init__()\n",
    "        self.detr = detr\n",
    "\n",
    "        if freeze_detr:\n",
    "            for p in self.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead\n",
    "        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0.0)\n",
    "        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        if isinstance(samples, (list, torch.Tensor)):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.detr.backbone(samples)\n",
    "\n",
    "        bs = features[-1].tensors.shape[0]\n",
    "\n",
    "        src, mask = features[-1].decompose()\n",
    "        assert mask is not None\n",
    "        src_proj = self.detr.input_proj(src)\n",
    "        hs, memory = self.detr.transformer(src_proj, mask, self.detr.query_embed.weight, pos[-1])\n",
    "\n",
    "        outputs_class = self.detr.class_embed(hs)\n",
    "        outputs_coord = self.detr.bbox_embed(hs).sigmoid()\n",
    "        out = {\"pred_logits\": outputs_class[-1], \"pred_boxes\": outputs_coord[-1]}\n",
    "        if self.detr.aux_loss:\n",
    "            out['aux_outputs'] = self.detr._set_aux_loss(outputs_class, outputs_coord)\n",
    "\n",
    "        # h_boxes takes the last one computed, keep this in mind\n",
    "        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)\n",
    "\n",
    "        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])\n",
    "        outputs_seg_masks = seg_masks.view(bs, self.detr.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
    "\n",
    "        out[\"pred_masks\"] = outputs_seg_masks\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7fefde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(inputs, targets, num_boxes):\n",
    "    \"\"\"\n",
    "    Compute the DICE loss, similar to generalized IOU for masks\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "    \"\"\"\n",
    "    inputs = inputs.sigmoid()\n",
    "    inputs = inputs.flatten(1)\n",
    "    numerator = 2 * (inputs * targets).sum(1)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
    "    return loss.sum() / num_boxes\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = -1 (no weighting).\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    \"\"\"\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    return loss.mean(1).sum() / num_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9176df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessSegm(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
    "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
    "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
    "        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n",
    "        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
    "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
    "\n",
    "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
    "            img_h, img_w = t[0], t[1]\n",
    "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
    "            results[i][\"masks\"] = F.interpolate(\n",
    "                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n",
    "            ).byte()\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f423c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessPanoptic(nn.Module):\n",
    "    \"\"\"This class converts the output of the model to the final panoptic result, in the format expected by the\n",
    "    coco panoptic API \"\"\"\n",
    "\n",
    "    def __init__(self, is_thing_map, threshold=0.85):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "           is_thing_map: This is a whose keys are the class ids, and the values a boolean indicating whether\n",
    "                          the class is  a thing (True) or a stuff (False) class\n",
    "           threshold: confidence threshold: segments with confidence lower than this will be deleted\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.is_thing_map = is_thing_map\n",
    "\n",
    "    def forward(self, outputs, processed_sizes, target_sizes=None):\n",
    "        \"\"\" This function computes the panoptic prediction from the model's predictions.\n",
    "        Parameters:\n",
    "            outputs: This is a dict coming directly from the model. See the model doc for the content.\n",
    "            processed_sizes: This is a list of tuples (or torch tensors) of sizes of the images that were passed to the\n",
    "                             model, ie the size after data augmentation but before batching.\n",
    "            target_sizes: This is a list of tuples (or torch tensors) corresponding to the requested final size\n",
    "                          of each prediction. If left to None, it will default to the processed_sizes\n",
    "            \"\"\"\n",
    "        if target_sizes is None:\n",
    "            target_sizes = processed_sizes\n",
    "        assert len(processed_sizes) == len(target_sizes)\n",
    "        out_logits, raw_masks, raw_boxes = outputs[\"pred_logits\"], outputs[\"pred_masks\"], outputs[\"pred_boxes\"]\n",
    "        assert len(out_logits) == len(raw_masks) == len(target_sizes)\n",
    "        preds = []\n",
    "\n",
    "        def to_tuple(tup):\n",
    "            if isinstance(tup, tuple):\n",
    "                return tup\n",
    "            return tuple(tup.cpu().tolist())\n",
    "\n",
    "        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n",
    "                out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes\n",
    "        ):\n",
    "            # we filter empty queries and detection below threshold\n",
    "            scores, labels = cur_logits.softmax(-1).max(-1)\n",
    "            keep = labels.ne(outputs[\"pred_logits\"].shape[-1] - 1) & (scores > self.threshold)\n",
    "            cur_scores, cur_classes = cur_logits.softmax(-1).max(-1)\n",
    "            cur_scores = cur_scores[keep]\n",
    "            cur_classes = cur_classes[keep]\n",
    "            cur_masks = cur_masks[keep]\n",
    "            cur_masks = interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n",
    "            cur_boxes = box_cxcywh_to_xyxy(cur_boxes[keep])\n",
    "\n",
    "            h, w = cur_masks.shape[-2:]\n",
    "            assert len(cur_boxes) == len(cur_classes)\n",
    "\n",
    "            # It may be that we have several predicted masks for the same stuff class.\n",
    "            # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n",
    "            cur_masks = cur_masks.flatten(1)\n",
    "            stuff_equiv_classes = defaultdict(lambda: [])\n",
    "            for k, label in enumerate(cur_classes):\n",
    "                if not self.is_thing_map[label.item()]:\n",
    "                    stuff_equiv_classes[label.item()].append(k)\n",
    "\n",
    "            def get_ids_area(masks, scores, dedup=False):\n",
    "                # This helper function creates the final panoptic segmentation image\n",
    "                # It also returns the area of the masks that appears on the image\n",
    "\n",
    "                m_id = masks.transpose(0, 1).softmax(-1)\n",
    "\n",
    "                if m_id.shape[-1] == 0:\n",
    "                    # We didn't detect any mask :(\n",
    "                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n",
    "                else:\n",
    "                    m_id = m_id.argmax(-1).view(h, w)\n",
    "\n",
    "                if dedup:\n",
    "                    # Merge the masks corresponding to the same stuff class\n",
    "                    for equiv in stuff_equiv_classes.values():\n",
    "                        if len(equiv) > 1:\n",
    "                            for eq_id in equiv:\n",
    "                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n",
    "\n",
    "                final_h, final_w = to_tuple(target_size)\n",
    "\n",
    "                seg_img = Image.fromarray(id2rgb(m_id.view(h, w).cpu().numpy()))\n",
    "                seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)\n",
    "\n",
    "                np_seg_img = (\n",
    "                    torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes())).view(final_h, final_w, 3).numpy()\n",
    "                )\n",
    "                m_id = torch.from_numpy(rgb2id(np_seg_img))\n",
    "\n",
    "                area = []\n",
    "                for i in range(len(scores)):\n",
    "                    area.append(m_id.eq(i).sum().item())\n",
    "                return area, seg_img\n",
    "\n",
    "            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n",
    "            if cur_classes.numel() > 0:\n",
    "                # We know filter empty masks as long as we find some\n",
    "                while True:\n",
    "                    filtered_small = torch.as_tensor(\n",
    "                        [area[i] <= 4 for i, c in enumerate(cur_classes)], dtype=torch.bool, device=keep.device\n",
    "                    )\n",
    "                    if filtered_small.any().item():\n",
    "                        cur_scores = cur_scores[~filtered_small]\n",
    "                        cur_classes = cur_classes[~filtered_small]\n",
    "                        cur_masks = cur_masks[~filtered_small]\n",
    "                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)\n",
    "\n",
    "            segments_info = []\n",
    "            for i, a in enumerate(area):\n",
    "                cat = cur_classes[i].item()\n",
    "                segments_info.append({\"id\": i, \"isthing\": self.is_thing_map[cat], \"category_id\": cat, \"area\": a})\n",
    "            del cur_classes\n",
    "\n",
    "            with io.BytesIO() as out:\n",
    "                seg_img.save(out, format=\"PNG\")\n",
    "                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n",
    "            preds.append(predictions)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb75eae",
   "metadata": {},
   "source": [
    "## conditional_detr File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b5f0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDETR(nn.Module):\n",
    "    \"\"\" This is the Conditional DETR module that performs object detection \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, transformer, num_classes, num_queries, channel_point, aux_loss=False):\n",
    "        \"\"\" Initializes the model.\n",
    "        Parameters:\n",
    "            backbone: torch module of the backbone to be used. See backbone.py\n",
    "            transformer: torch module of the transformer architecture. See transformer.py\n",
    "            num_classes: number of object classes\n",
    "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
    "                         Conditional DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
    "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "        self.transformer = transformer\n",
    "        hidden_dim = transformer.d_model\n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes)\n",
    "        self.point_embed = MLP(hidden_dim, hidden_dim, channel_point, 3)\n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
    "        self.backbone = backbone\n",
    "        self.aux_loss = aux_loss\n",
    "\n",
    "        # init prior_prob setting for focal loss\n",
    "        prior_prob = 0.01\n",
    "        bias_value = -math.log((1 - prior_prob) / prior_prob)\n",
    "        self.class_embed.bias.data = torch.ones(num_classes) * bias_value\n",
    "\n",
    "        # init point_mebed\n",
    "        nn.init.constant_(self.point_embed.layers[-1].weight.data, 0)\n",
    "        nn.init.constant_(self.point_embed.layers[-1].bias.data, 0)\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        \"\"\"The forward expects a NestedTensor, which consists of:\n",
    "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
    "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
    "\n",
    "            It returns a dict with the following elements:\n",
    "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
    "                                Shape= [batch_size x num_queries x num_classes]\n",
    "               - \"pred_points\": The normalized points coordinates for all queries, represented as\n",
    "                               (center_x, center_y, width, height). These values are normalized in [0, 1],\n",
    "                               relative to the size of each individual image (disregarding possible padding).\n",
    "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
    "               - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
    "                                dictionnaries containing the two above keys for each decoder layer.\n",
    "        \"\"\"\n",
    "        if isinstance(samples, (list, torch.Tensor)):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.backbone(samples)\n",
    "\n",
    "        src, mask = features[-1].decompose()\n",
    "        assert mask is not None\n",
    "        hs, reference = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])\n",
    "\n",
    "        reference_before_sigmoid = inverse_sigmoid(reference)\n",
    "        outputs_coords = []\n",
    "        for lvl in range(hs.shape[0]):\n",
    "            tmp = self.point_embed(hs[lvl])\n",
    "            tmp[..., :2] += reference_before_sigmoid\n",
    "            outputs_coord = tmp.sigmoid()\n",
    "            outputs_coords.append(outputs_coord)\n",
    "        outputs_coord = torch.stack(outputs_coords)\n",
    "\n",
    "        outputs_class = self.class_embed(hs)\n",
    "        out = {'pred_logits': outputs_class[-1], 'pred_points': outputs_coord[-1]}\n",
    "        if self.aux_loss:\n",
    "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
    "        return out\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
    "        # this is a workaround to make torchscript happy, as torchscript\n",
    "        # doesn't support dictionary with non-homogeneous values, such\n",
    "        # as a dict having both a Tensor and a list.\n",
    "        return [{'pred_logits': a, 'pred_points': b}\n",
    "                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bda0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" This class computes the loss for Conditional DETR.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth points and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, matcher, weight_dict, focal_alpha, losses):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "            focal_alpha: alpha in Focal Loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.losses = losses\n",
    "        self.focal_alpha = focal_alpha\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_points, log=True):\n",
    "        \"\"\"Classification loss (Binary focal loss)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_points]\n",
    "        \"\"\"\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)]).cuda()\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        target_classes_onehot = torch.zeros([src_logits.shape[0], src_logits.shape[1], src_logits.shape[2] + 1],\n",
    "                                            dtype=src_logits.dtype, layout=src_logits.layout, device=src_logits.device)\n",
    "        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n",
    "\n",
    "        target_classes_onehot = target_classes_onehot[:, :, :-1]\n",
    "        loss_ce = sigmoid_focal_loss(src_logits, target_classes_onehot, num_points, alpha=self.focal_alpha, gamma=2) * \\\n",
    "                  src_logits.shape[1]\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        if log:\n",
    "            # this should probably be a separate loss, not hacked in this one here\n",
    "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_points):\n",
    "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty points\n",
    "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_points(self, outputs, targets, indices, num_points):\n",
    "        \"\"\"Compute the losses related to the bounding points, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"points\" containing a tensor of dim [nb_target_points, 4]\n",
    "           The target points are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
    "        \"\"\"\n",
    "        assert 'pred_points' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_points = outputs['pred_points'][idx]\n",
    "        target_points = torch.cat([t['points'][i] for t, (_, i) in zip(targets, indices)], dim=0).cuda()\n",
    "\n",
    "        loss_point = F.l1_loss(src_points, target_points, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_point'] = loss_point.sum() / num_points\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def loss_masks(self, outputs, targets, indices, num_points):\n",
    "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_points, h, w]\n",
    "        \"\"\"\n",
    "        assert \"pred_masks\" in outputs\n",
    "\n",
    "        src_idx = self._get_src_permutation_idx(indices)\n",
    "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
    "        src_masks = outputs[\"pred_masks\"]\n",
    "        src_masks = src_masks[src_idx]\n",
    "        masks = [t[\"masks\"] for t in targets]\n",
    "        # use valid to mask invalid areas due to padding in loss\n",
    "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
    "        target_masks = target_masks.to(src_masks)\n",
    "        target_masks = target_masks[tgt_idx]\n",
    "\n",
    "        # upsample predictions to the target size\n",
    "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
    "                                mode=\"bilinear\", align_corners=False)\n",
    "        src_masks = src_masks[:, 0].flatten(1)\n",
    "\n",
    "        target_masks = target_masks.flatten(1)\n",
    "        target_masks = target_masks.view(src_masks.shape)\n",
    "        losses = {\n",
    "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_points),\n",
    "            \"loss_dice\": dice_loss(src_masks, target_masks, num_points),\n",
    "        }\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_points, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'points': self.loss_points,\n",
    "            'masks': self.loss_masks\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_points, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        # Compute the average number of target points accross all nodes, for normalization purposes\n",
    "        num_points = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_points = torch.as_tensor([num_points], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        if dist_utils.is_dist_avail_and_initialized():\n",
    "            torch.distributed.all_reduce(num_points)\n",
    "        num_points = torch.clamp(num_points / dist_utils.get_world_size(), min=1).item()\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_points))\n",
    "\n",
    "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
    "                indices = self.matcher(aux_outputs, targets)\n",
    "                for loss in self.losses:\n",
    "                    if loss == 'masks':\n",
    "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
    "                        continue\n",
    "                    kwargs = {}\n",
    "                    if loss == 'labels':\n",
    "                        # Logging is enabled only for the last layer\n",
    "                        kwargs = {'log': False}\n",
    "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_points, **kwargs)\n",
    "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91320ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcess(nn.Module):\n",
    "    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        \"\"\" Perform the computation\n",
    "        Parameters:\n",
    "            outputs: raw outputs of the model\n",
    "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
    "                          For evaluation, this must be the original image size (before any data augmentation)\n",
    "                          For visualization, this should be the image size after data augment, but before padding\n",
    "        \"\"\"\n",
    "        out_logits, out_point = outputs['pred_logits'], outputs['pred_points']\n",
    "\n",
    "        assert len(out_logits) == len(target_sizes)\n",
    "        assert target_sizes.shape[1] == 2\n",
    "\n",
    "        prob = out_logits.sigmoid()\n",
    "        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n",
    "        scores = topk_values\n",
    "        topk_points = topk_indexes // out_logits.shape[2]\n",
    "        labels = topk_indexes % out_logits.shape[2]\n",
    "        points = box_cxcywh_to_xyxy(out_point)\n",
    "        points = torch.gather(points, 1, topk_points.unsqueeze(-1).repeat(1, 1, 4))\n",
    "\n",
    "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "        img_h, img_w = target_sizes.unbind(1)\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
    "        points = points * scale_fct[:, None, :]\n",
    "\n",
    "        results = [{'scores': s, 'labels': l, 'points': b} for s, l, b in zip(scores, labels, points)]\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3437d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6da6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(args):\n",
    "    # the `num_classes` naming here is somewhat misleading.\n",
    "    # it indeed corresponds to `max_obj_id + 1`, where max_obj_id\n",
    "    # is the maximum id for a class in your dataset. For example,\n",
    "    # COCO has a max_obj_id of 90, so we pass `num_classes` to be 91.\n",
    "    # As another example, for a dataset that has a single class with id 1,\n",
    "    # you should pass `num_classes` to be 2 (max_obj_id + 1).\n",
    "    # For more details on this, check the following discussion\n",
    "    # https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\n",
    "    num_classes = 2 if args.dataset_file != 'coco' else 91\n",
    "    if args.dataset_file == \"coco_panoptic\":\n",
    "        # for panoptic, we just add a num_classes that is large enough to hold\n",
    "        # max_obj_id + 1, but the exact value doesn't really matter\n",
    "        num_classes = 250\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    backbone = build_backbone(args)\n",
    "\n",
    "    transformer = build_transformer(args)\n",
    "\n",
    "    model = ConditionalDETR(\n",
    "        backbone,\n",
    "        transformer,\n",
    "        num_classes=num_classes,\n",
    "        num_queries=args.num_queries,\n",
    "        channel_point = args.channel_point,\n",
    "        aux_loss=args.aux_loss,\n",
    "    )\n",
    "\n",
    "    if args.masks:\n",
    "        model = DETRsegm(model, freeze_detr=(args.frozen_weights is not None))\n",
    "    matcher = build_matcher(args)\n",
    "    weight_dict = {'loss_ce': args.cls_loss_coef, 'loss_point': args.point_loss_coef}\n",
    "    weight_dict['loss_giou'] = args.giou_loss_coef\n",
    "    if args.masks:\n",
    "        weight_dict[\"loss_mask\"] = args.mask_loss_coef\n",
    "        weight_dict[\"loss_dice\"] = args.dice_loss_coef\n",
    "    # this is a hack\n",
    "    if args.aux_loss:\n",
    "        aux_weight_dict = {}\n",
    "        for i in range(args.dec_layers - 1):\n",
    "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
    "        weight_dict.update(aux_weight_dict)\n",
    "\n",
    "    losses = ['labels', 'points', 'cardinality']\n",
    "    if args.masks:\n",
    "        losses += [\"masks\"]\n",
    "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict,\n",
    "                             focal_alpha=args.focal_alpha, losses=losses)\n",
    "    criterion.to(device)\n",
    "    postprocessors = {'point': PostProcess()}\n",
    "    if args.masks:\n",
    "        postprocessors['segm'] = PostProcessSegm()\n",
    "        if args.dataset_file == \"coco_panoptic\":\n",
    "            is_thing_map = {i: i <= 90 for i in range(201)}\n",
    "            postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
    "\n",
    "    return model, criterion, postprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08c722",
   "metadata": {},
   "source": [
    "## init File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5084a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "\n",
    "    model, criterion, postprocessors = build(args)\n",
    "\n",
    "    return model, criterion, postprocessors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
