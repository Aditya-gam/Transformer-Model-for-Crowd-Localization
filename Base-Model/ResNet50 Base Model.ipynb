{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy matplotlib scipy pandas scikit-learn seaborn statsmodels torch torchvision transformers opencv-python Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time\n",
    "import scipy\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from transformers import TransformerEncoder, TransformerDecoder\n",
    "from transformers import TransformerEncoderLayer, TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for crowd counting, which handles loading and preprocessing of images and annotations.\n",
    "\n",
    "    Attributes:\n",
    "        image_paths (list): A list of paths to the image files.\n",
    "        annotation_paths (list): A list of paths to the annotation files.\n",
    "        crop_size (tuple): The target size (height, width) for random cropping.\n",
    "        scale (tuple): The range of scaling factors for random scaling.\n",
    "        flip_prob (float): Probability of horizontally flipping the image.\n",
    "        max_size (int): Maximum size for resizing the images during testing.\n",
    "        phase (str): The current phase of the dataset, 'train' or 'test'.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, annotation_paths, crop_size, scale, flip_prob, max_size, phase):\n",
    "        \"\"\"\n",
    "        Initializes the CrowdDataset object with the necessary parameters and setups up the transformation pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "            image_paths (list of str): Paths to the image files.\n",
    "            annotation_paths (list of str): Paths to the annotation files.\n",
    "            crop_size (tuple of int): Desired output size of the cropped images as (height, width).\n",
    "            scale (tuple of float): The range (min, max) of scaling factors for image scaling.\n",
    "            flip_prob (float): Probability of flipping the image horizontally during augmentation.\n",
    "            max_size (int): Maximum size of the images during testing for resizing.\n",
    "            phase (str): Specifies the dataset phase ('train' or 'test') to tailor transformations accordingly.\n",
    "        \"\"\"\n",
    "        # Store the paths to the images and annotations\n",
    "        self.image_paths = image_paths\n",
    "        self.annotation_paths = annotation_paths\n",
    "\n",
    "        # Store image processing parameters\n",
    "        self.crop_size = crop_size  # Desired crop size for images\n",
    "        self.scale = scale  # Range of scale for scaling the images\n",
    "        self.flip_prob = flip_prob  # Probability of flipping an image\n",
    "        self.max_size = max_size  # Maximum size for resizing in the test phase\n",
    "        self.phase = phase  # Dataset phase (training or testing)\n",
    "\n",
    "        # Set up a sequence of transformations for image preprocessing\n",
    "        # This includes converting images to tensors and normalizing pixel values\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize using the mean and std of the ImageNet dataset\n",
    "                                std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of images available in the dataset.\n",
    "        \"\"\"\n",
    "        # Return the length of the list that contains image paths, representing the number of images\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve an image and its corresponding annotations at a specified index with appropriate preprocessing applied.\n",
    "\n",
    "        Parameters:\n",
    "            idx (int): The index of the image and annotations to retrieve from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the processed image as a tensor and the corresponding points as a tensor.\n",
    "        \"\"\"\n",
    "        # Open the image file at the given index, convert it to an RGB image\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        # Load the annotation points for the corresponding image\n",
    "        points = np.load(self.annotation_paths[idx])\n",
    "        \n",
    "        # Check the phase of the dataset to determine the preprocessing steps\n",
    "        if self.phase == 'train':\n",
    "            # In training phase, perform random cropping, scaling, and flipping\n",
    "            image, points = self.random_crop(image, points, self.crop_size)\n",
    "            image, points = self.random_scale(image, points, self.scale)\n",
    "            image, points = self.random_flip(image, points, self.flip_prob)\n",
    "        elif self.phase == 'test':\n",
    "            # In testing phase, only resize the image to maintain consistency\n",
    "            image, points = self.resize_image(image, points, self.max_size)\n",
    "\n",
    "        # Apply transformations to the image (e.g., normalization, conversion to tensor)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Convert points to a tensor with the appropriate data type\n",
    "        return image, torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    def random_crop(self, image, points, crop_size):\n",
    "        \"\"\"\n",
    "        Randomly crops the image to a specified size and adjusts the annotation points to fit the new image dimensions.\n",
    "\n",
    "        Parameters:\n",
    "            image (PIL.Image): The image to be cropped.\n",
    "            points (numpy.ndarray): Array of annotation points associated with the image.\n",
    "            crop_size (tuple of int): The desired height and width (height, width) to crop the image to.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The cropped image and the adjusted points within the new image dimensions.\n",
    "        \"\"\"\n",
    "        # Determine the current width and height of the image\n",
    "        w, h = image.size\n",
    "        new_h, new_w = crop_size  # Unpack the desired crop dimensions\n",
    "        \n",
    "        # Randomly select a top coordinate for cropping if the current height is greater than the desired height\n",
    "        if h > new_h:\n",
    "            top = np.random.randint(0, h - new_h)\n",
    "        else:\n",
    "            top = 0  # If the image is smaller than or equal to the desired crop height, start at the top\n",
    "        \n",
    "        # Randomly select a left coordinate for cropping if the current width is greater than the desired width\n",
    "        if w > new_w:\n",
    "            left = np.random.randint(0, w - new_w)\n",
    "        else:\n",
    "            left = 0  # If the image is smaller than or equal to the desired crop width, start at the left\n",
    "\n",
    "        # Crop the image from the calculated top and left points to the new width and height\n",
    "        image = image.crop((left, top, left + new_w, top + new_h))\n",
    "        # Adjust the points by subtracting the left and top offset, effectively shifting them to the new coordinate system\n",
    "        points = points - [left, top]\n",
    "        # Filter out points that are now outside the bounds of the new image dimensions\n",
    "        points = points[(points[:, 0] >= 0) & (points[:, 1] >= 0) & (points[:, 0] < new_w) & (points[:, 1] < new_h)]\n",
    "        \n",
    "        return image, points\n",
    "\n",
    "    \n",
    "    def random_scale(self, image, points, scale_range):\n",
    "        \"\"\"\n",
    "        Randomly scales the image and its associated annotation points according to a specified scale range.\n",
    "\n",
    "        Parameters:\n",
    "            image (PIL.Image): The image to be scaled.\n",
    "            points (numpy.ndarray): Array of annotation points associated with the image.\n",
    "            scale_range (tuple of float): A tuple containing the minimum and maximum scaling factors.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The scaled image and the adjusted points as per the scale factor applied.\n",
    "        \"\"\"\n",
    "        # Randomly choose a scale factor within the given range\n",
    "        scale_factor = random.uniform(*scale_range)\n",
    "        \n",
    "        # Apply the affine transformation to scale the image without rotation or shearing\n",
    "        image = transforms.functional.affine(image, angle=0, translate=(0, 0), scale=scale_factor, shear=0)\n",
    "        \n",
    "        # Scale the annotation points to match the new image size\n",
    "        points *= scale_factor\n",
    "        \n",
    "        return image, points\n",
    "\n",
    "    \n",
    "    def random_flip(self, image, points, prob):\n",
    "        \"\"\"\n",
    "        Horizontally flips the image with a given probability and adjusts the annotation points accordingly.\n",
    "\n",
    "        Parameters:\n",
    "            image (PIL.Image): The image to potentially flip.\n",
    "            points (numpy.ndarray): Array of annotation points associated with the image.\n",
    "            prob (float): Probability of flipping the image horizontally.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The possibly flipped image and the appropriately adjusted points.\n",
    "        \"\"\"\n",
    "        # Check if the image should be flipped based on a random probability\n",
    "        if random.random() < prob:\n",
    "            # Horizontally flip the image\n",
    "            image = transforms.functional.hflip(image)\n",
    "            # Get the width of the image to calculate the new positions of the points\n",
    "            w, _ = image.size\n",
    "            # Reflect the x-coordinates of the points across the width of the image\n",
    "            points[:, 0] = w - points[:, 0]\n",
    "        \n",
    "        return image, points\n",
    "\n",
    "\n",
    "    def resize_image(self, image, points, max_size):\n",
    "        \"\"\"\n",
    "        Resizes the image to ensure its largest dimension does not exceed the specified maximum size, \n",
    "        and adjusts the annotation points accordingly.\n",
    "\n",
    "        Parameters:\n",
    "            image (PIL.Image): The image to be resized.\n",
    "            points (numpy.ndarray): Array of annotation points associated with the image.\n",
    "            max_size (int): The maximum size that the image's largest dimension should not exceed.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The resized image and the adjusted points.\n",
    "        \"\"\"\n",
    "        # Get current width and height of the image\n",
    "        w, h = image.size\n",
    "        # Check if the largest dimension of the image exceeds the maximum size\n",
    "        if max(h, w) > max_size:\n",
    "            # Calculate the ratio to scale down to the maximum size\n",
    "            ratio = max_size / max(h, w)\n",
    "            # Resize the image using the computed ratio\n",
    "            image = transforms.functional.resize(image, (int(h * ratio), int(w * ratio)))\n",
    "            # Adjust the points by the same ratio to match the new image size\n",
    "            points *= ratio\n",
    "\n",
    "        return image, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_backbone(pretrained=True):\n",
    "    \"\"\"\n",
    "    Creates a modified ResNet50 model to be used as a backbone in other models, \n",
    "    where the final fully connected layer and the average pooling layer are removed.\n",
    "\n",
    "    Parameters:\n",
    "        pretrained (bool): If True, loads a ResNet50 model pre-trained on ImageNet. \n",
    "                           If False, initializes a new ResNet50 model without pre-trained weights.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Sequential: The modified ResNet50 model without the final classification layers.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained ResNet50 model if specified, otherwise load a default new model\n",
    "    model = resnet50(pretrained=pretrained)\n",
    "    # Remove the final two layers (average pooling and fully connected layer) to use as a feature extractor\n",
    "    backbone = torch.nn.Sequential(*(list(model.children())[:-2]))\n",
    "    return backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds positional encodings to the input embeddings to introduce a notion of word order.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (int): The dimensionality of the input embeddings.\n",
    "        max_len (int): The maximum length of the input sequences for which positional encodings will be generated.\n",
    "\n",
    "    Methods:\n",
    "        forward(feature_size): Applies positional encoding up to the specified feature size.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Initializes the PositionalEncoding module with a specific embedding dimension and maximum sequence length.\n",
    "\n",
    "        Parameters:\n",
    "            d_model (int): The dimensionality of the model's input embeddings.\n",
    "            max_len (int): The maximum length of the sequences for which positional encodings are to be created.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Initialize a zero matrix for positional encodings\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        # Generate a position array from 0 to max_len\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # Calculate the division term for the encoding formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        # Apply sine to even indices in the positional encoding matrix\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in the positional encoding matrix\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Add a batch dimension\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, feature_size):\n",
    "        \"\"\"\n",
    "        Retrieve the positional encoding for the first 'feature_size' positions.\n",
    "\n",
    "        Parameters:\n",
    "            feature_size (int): The number of positions to retrieve encodings for, \n",
    "                                typically the length of the input sequences.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The positional encodings for the specified feature size.\n",
    "        \"\"\"\n",
    "        # Return positional encodings up to the requested feature size\n",
    "        return self.encoding[:, :feature_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer-based model designed for tasks that require encoding and decoding capabilities, enhanced\n",
    "    with positional encoding to maintain sequence order awareness.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (int): The number of expected features in the transformer's input and output.\n",
    "        nhead (int): The number of heads in the multihead attention models.\n",
    "        num_encoder_layers (int): The number of sub-encoder-layers in the transformer encoder.\n",
    "        num_decoder_layers (int): The number of sub-decoder-layers in the transformer decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model=256, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        \"\"\"\n",
    "        Initializes the CrowdTransformer model with specified configurations for the encoder and decoder.\n",
    "\n",
    "        Parameters:\n",
    "            d_model (int): The number of expected features in the input (also the size of embeddings).\n",
    "            nhead (int): The number of heads in the multihead attention mechanism.\n",
    "            num_encoder_layers (int): The number of transformer encoder layers.\n",
    "            num_decoder_layers (int): The number of transformer decoder layers.\n",
    "        \"\"\"\n",
    "        super(CrowdTransformer, self).__init__()\n",
    "        # Define the encoder layer and repeat it 'num_encoder_layers' times in the encoder\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        # Define the decoder layer and repeat it 'num_decoder_layers' times in the decoder\n",
    "        decoder_layers = TransformerDecoderLayer(d_model, nhead)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, num_decoder_layers)\n",
    "        \n",
    "        # Add positional encoding to inject some information about the relative or absolute position of the tokens\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "    def forward(self, src, queries, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Processes the input through the transformer model to generate outputs based on source and target queries.\n",
    "\n",
    "        Parameters:\n",
    "            src (Tensor): The sequence to the encoder (source sequence).\n",
    "            queries (Tensor): The sequence to the decoder (target queries).\n",
    "            src_key_padding_mask (Tensor, optional): The mask for the src keys per batch (optional).\n",
    "            tgt_key_padding_mask (Tensor, optional): The mask for the tgt keys per batch (optional).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output from the transformer decoder.\n",
    "        \"\"\"\n",
    "        # Apply positional encoding to the source sequence\n",
    "        src = self.positional_encoding(src)\n",
    "        # Encode the source sequence\n",
    "        memory = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        # Decode the encoded source along with the target queries\n",
    "        output = self.transformer_decoder(queries, memory, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMOMatcher(nn.Module):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdLocalizationLoss(nn.Module):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLTR(nn.Module):\n",
    "    def __init__(self, num_queries=500, ...):\n",
    "        super(CLTR, self).__init__()\n",
    "        self.backbone = create_resnet_backbone()\n",
    "        self.transformer = CrowdTransformer(...)\n",
    "        self.matcher = KMOMatcher(...)\n",
    "        self.loss = CrowdLocalizationLoss(...)\n",
    "        \n",
    "    def forward(self, images, targets=None):\n",
    "        # Feature extraction\n",
    "        features = self.backbone(images)\n",
    "        \n",
    "        # Flatten feature maps and combine with positional encodings\n",
    "        \n",
    "        # Prepare queries\n",
    "        \n",
    "        # Transformer forward pass\n",
    "        \n",
    "        # If training, use matcher and calculate loss\n",
    "        if self.training:\n",
    "            # Match predictions to ground truth\n",
    "            # Calculate loss\n",
    "            return loss\n",
    "        else:\n",
    "            # Return predictions for evaluation\n",
    "            return predictions\n",
    "        \n",
    "    # Implement additional methods if necessary, e.g., fit, predict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
