{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import List, Optional\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import io\n",
    "from panopticapi.utils import id2rgb, rgb2id\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_, constant_, xavier_normal_\n",
    "import torchvision\n",
    "from torchvision.models._utils import IntermediateLayerGetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "- **PositionEmbeddingSine**: It now calculates the sinusoidal embeddings based on the positions of elements in the image grid, suitable for transformers.\n",
    "- **PositionEmbeddingLearned**: Adjusted to initialize weights more robustly and provided parameters for max dimensions to accommodate different image sizes.\n",
    "- **build_position_encoding**: This function now supports creating either 'sine' or 'learned' embeddings based on the input arguments, making it flexible for different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"Sinusoidal Position Embedding for image-based inputs.\"\"\"\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        self.scale = scale if scale is not None else 2 * math.pi\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.mask is not None, \"Mask cannot be None\"\n",
    "        mask = x.mask\n",
    "        not_mask = ~mask\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "    \n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.tensors.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"Learned Position Embedding for image-based inputs.\"\"\"\n",
    "    def __init__(self, num_pos_feats=256, max_height=50, max_width=50):\n",
    "        super().__init__()\n",
    "        self.row_embed = nn.Embedding(max_height, num_pos_feats)\n",
    "        self.col_embed = nn.Embedding(max_width, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight, -0.05, 0.05)\n",
    "        nn.init.uniform_(self.col_embed.weight, -0.05, 0.05)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.mask is not None, \"Mask cannot be None\"\n",
    "        h, w = x.tensors.shape[-2:]\n",
    "        i = torch.arange(w, device=x.tensors.device)\n",
    "        j = torch.arange(h, device=x.tensors.device)\n",
    "        x_emb = self.col_embed(i)\n",
    "        y_emb = self.row_embed(j)\n",
    "        pos = torch.cat([x_emb.unsqueeze(0).repeat(h, 1, 1), y_emb.unsqueeze(1).repeat(1, w, 1)], dim=-1)\n",
    "        pos = pos.permute(2, 0, 1).unsqueeze(0).repeat(x.tensors.size(0), 1, 1, 1)\n",
    "        \n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_position_encoding(hidden_dim, position_embedding='sine', max_height=50, max_width=50):\n",
    "    \"\"\"Factory function to build position encoding.\"\"\"\n",
    "    num_pos_feats = hidden_dim // 2\n",
    "    if position_embedding == 'sine':\n",
    "        return PositionEmbeddingSine(num_pos_feats, normalize=True)\n",
    "    elif position_embedding == 'learned':\n",
    "        return PositionEmbeddingLearned(num_pos_feats, max_height, max_width)\n",
    "    else:\n",
    "        raise ValueError(f\"Not supported position embedding type {position_embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "This refactored code maintains the same overall structure but simplifies the definition and initialization of the backbone and its components. The `FrozenBatchNorm2d` function is streamlined by using a regular `BatchNorm2d` and setting the gradient flags to `False`. The `BackboneBase` is merged and simplified into `SimplifiedBackbone`, and the functionality of handling different returning layers and feature channels is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frozen_batch_norm2d(num_features, eps=1e-5):\n",
    "    \"\"\"Creates a frozen batch normalization layer.\"\"\"\n",
    "    layer = nn.BatchNorm2d(num_features, affine=True)\n",
    "    layer.weight.requires_grad = False\n",
    "    layer.bias.requires_grad = False\n",
    "    layer.running_mean.requires_grad = False\n",
    "    layer.running_var.requires_grad = False\n",
    "    layer.eps = eps\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device):\n",
    "        cast_tensor = self.tensors.to(device)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            assert mask is not None\n",
    "            cast_mask = mask.to(device)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBackbone(nn.Module):\n",
    "    \"\"\"Simplified backbone model for feature extraction.\"\"\"\n",
    "    def __init__(self, model_name, train_layers=False, num_channels=2048, return_interm_layers=False):\n",
    "        super().__init__()\n",
    "        backbone = getattr(torchvision.models, model_name)(pretrained=True, norm_layer=lambda num_features: frozen_batch_norm2d(num_features))\n",
    "\n",
    "        for name, parameter in backbone.named_parameters():\n",
    "            if not train_layers or all(not name.startswith(f'layer{i}') for i in [2, 3, 4]):\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "        return_layers = {f'layer{i}': str(i-1) for i in range(1, 5)} if return_interm_layers else {'layer4': '0'}\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.body(x.tensors)\n",
    "\n",
    "        return {name: NestedTensor(x, x.mask) for name, x in xs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneWithPositionEmbedding(nn.Sequential):\n",
    "    \"\"\"Combines a backbone and a position embedding module.\"\"\"\n",
    "    def __init__(self, backbone, position_embedding):\n",
    "        super().__init__(backbone, position_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self[0](x)\n",
    "        pos = [self[1](x) for x in xs.values()]\n",
    "        return xs, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_refactored_backbone(model_name, train_layers, num_channels, return_interm_layers, hidden_dim, position_embedding_type):\n",
    "    \"\"\"Builds a backbone with integrated position encoding.\"\"\"\n",
    "    position_embedding = build_position_encoding(hidden_dim, position_embedding_type)\n",
    "    backbone = SimplifiedBackbone(model_name, train_layers, num_channels, return_interm_layers)\n",
    "    return BackboneWithPositionEmbedding(backbone, position_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "This version of the Hungarian matcher will compute matching costs using classification scores, point distances, and optionally GIoU scores (if applicable). However, since the original paper discusses a KMO-based matcher, and your model doesn't include explicit GIoU computation for bounding boxes, we'll focus only on classification and point distances for simplicity.\n",
    "\n",
    "\n",
    "1. **Logits to Probabilities**: The matcher uses `softmax` to convert the logits to probabilities. This is crucial for calculating the classification cost as negative log likelihood, which is more stable and interpretable.\n",
    "2. **Cost Calculation**: The matcher combines classification and point costs linearly using specified weights. It supports batch processing where each element in the batch has its own set of targets and predictions.\n",
    "3. **Linear Sum Assignment**: This uses the `linear_sum_assignment` from `SciPy`, which directly finds the minimum cost matching. It's applied batch-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"Computes an assignment between predictions and ground truth targets.\"\"\"\n",
    "    def __init__(self, cost_class: float = 1.0, cost_point: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_point = cost_point\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            outputs: Dict containing at least 'pred_logits' and 'pred_points'.\n",
    "                - 'pred_logits': Tensor of shape [batch_size, num_queries, num_classes]\n",
    "                - 'pred_points': Tensor of shape [batch_size, num_queries, 4] (for bounding box points)\n",
    "            targets: List of dictionaries containing:\n",
    "                - 'labels': Tensor of shape [num_target_points]\n",
    "                - 'points': Tensor of shape [num_target_points, 4]\n",
    "\n",
    "        Returns:\n",
    "            List of tuples for each batch element, containing:\n",
    "            - Index of selected predictions.\n",
    "            - Index of corresponding selected targets.\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs['pred_logits'].shape[:2]\n",
    "        out_prob = outputs['pred_logits'].softmax(-1)  # Convert logits to probabilities\n",
    "        out_points = outputs['pred_points'].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Concatenate all targets across the batch\n",
    "        tgt_points = torch.cat([t['points'] for t in targets]).to(out_points.device)\n",
    "        tgt_labels = torch.cat([t['labels'] for t in targets])\n",
    "\n",
    "        # Compute classification cost using negative log likelihood\n",
    "        cost_class = -out_prob[:, tgt_labels].flatten(0, 1)\n",
    "\n",
    "        # Compute L1 cost between predicted points and target points\n",
    "        cost_point = torch.cdist(out_points, tgt_points, p=1)\n",
    "\n",
    "        # Combine costs\n",
    "        C = self.cost_class * cost_class + self.cost_point * cost_point\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        # Compute assignment for each batch element\n",
    "        indices = [linear_sum_assignment(c) for c in C]\n",
    "        return [(torch.as_tensor(i, dtype=torch.long), torch.as_tensor(j, dtype=torch.long)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matcher(cost_class=1.0, cost_point=1.0):\n",
    "    return HungarianMatcher(cost_class, cost_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes Made:\n",
    "\n",
    "This `MultiheadAttention`:\n",
    "- Directly uses the input dimensions of query, key, and value without additional projection, simplifying the architecture.\n",
    "- Uses scaled dot-product attention mechanism, following the principle outlined in \"Attention is All You Need\".\n",
    "- Provides dropout for regularization and a final linear projection to match the output dimensions to the input dimensions.\n",
    "- Is flexible with regard to whether it returns attention weights, allowing for easier debugging or further manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"Custom implementation of MultiheadAttention to support different dimensions\n",
    "    for query, key, and value without separate projection matrices for each.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameter initialization\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        xavier_uniform_(self.out_proj.weight)\n",
    "        if self.out_proj.bias is not None:\n",
    "            constant_(self.out_proj.bias, 0)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None):\n",
    "        \"\"\" Forward pass for custom multihead attention.\n",
    "        Assumes L = target sequence length, S = source sequence length, N = batch size, E = embedding dimension.\n",
    "        \"\"\"\n",
    "        tgt_len, bsz, embed_dim = query.size()\n",
    "        assert query.size() == key.size() == value.size(), \"Query, Key and Value must be of the same size\"\n",
    "\n",
    "        # Scale query for dot product attention\n",
    "        scaling = self.head_dim ** -0.5\n",
    "        query = query * scaling\n",
    "\n",
    "        # Calculate Q, K, V\n",
    "        q = query.reshape(tgt_len, bsz * self.num_heads, self.head_dim)\n",
    "        k = key.reshape(-1, bsz * self.num_heads, self.head_dim)\n",
    "        v = value.reshape(-1, bsz * self.num_heads, self.head_dim)\n",
    "\n",
    "        # Dot product of Q and K (transpose)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, -1)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf')\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * self.num_heads, tgt_len, -1)\n",
    "\n",
    "        # Apply softmax and dropout on attention weights\n",
    "        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "        attn_output_weights = self.dropout_layer(attn_output_weights)\n",
    "\n",
    "        # Multiply weights by V\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "\n",
    "        # Transpose and reshape to bring back to original dimensions\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "\n",
    "        # Apply final linear projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        if need_weights:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, -1)\n",
    "            attn_output_weights = attn_output_weights.sum(dim=1) / self.num_heads\n",
    "        else:\n",
    "            attn_output_weights = None\n",
    "\n",
    "        return attn_output, attn_output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    \"\"\"Create N identical layers by deep copying the given module.\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    elif activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(\"Activation function {} is not supported\".format(activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, layer, n_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(layer, n_layers)\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.embed_dim)\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        src = self.norm(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, layer, n_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(layer, n_layers)\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.embed_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        tgt = self.norm(tgt)\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\" Container module hosting the encoder and decoder. \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, pos_embed):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.pos_embed = pos_embed\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        memory = self.encoder(self.pos_embed(self.src_embed(src)), src_mask, src_key_padding_mask)\n",
    "        output = self.decoder(self.pos_embed(self.tgt_embed(tgt)), memory, tgt_mask, None, tgt_key_padding_mask, None)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation='relu'):\n",
    "    \"\"\"Function to build the transformer model.\"\"\"\n",
    "    encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "    encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "    \n",
    "    decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "    decoder = TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "    \n",
    "    src_embed = nn.Embedding(num_embeddings=10, embedding_dim=d_model)  # Placeholder for actual embedding logic\n",
    "    tgt_embed = nn.Embedding(num_embeddings=10, embedding_dim=d_model)  # Placeholder for actual embedding logic\n",
    "    pos_encoder = PositionalEncoding(d_model=d_model)\n",
    "    \n",
    "    return TransformerModel(encoder, decoder, src_embed, tgt_embed, pos_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMap(nn.Module):\n",
    "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        nn.init.zeros_(self.k_linear.bias)\n",
    "        nn.init.zeros_(self.q_linear.bias)\n",
    "        nn.init.xavier_uniform_(self.k_linear.weight)\n",
    "        nn.init.xavier_uniform_(self.q_linear.weight)\n",
    "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, q, k, mask: Optional[Tensor] = None):\n",
    "        q = self.q_linear(q)\n",
    "        k = F.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
    "        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
    "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", qh * self.normalize_fact, kh)\n",
    "\n",
    "        if mask is not None:\n",
    "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "        weights = F.softmax(weights.flatten(2), dim=-1).view(weights.size())\n",
    "        weights = self.dropout(weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalMaskHead(nn.Module):\n",
    "    def __init__(self, dim, fpn_dims, context_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n",
    "        self.lay1 = nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(8, dim)\n",
    "        self.lay2 = nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(8, inter_dims[1])\n",
    "        self.lay3 = nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
    "        self.gn3 = nn.GroupNorm(8, inter_dims[2])\n",
    "        self.lay4 = nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
    "        self.gn4 = nn.GroupNorm(8, inter_dims[3])\n",
    "        self.lay5 = nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
    "        self.gn5 = nn.GroupNorm(8, inter_dims[4])\n",
    "        self.out_lay = nn.Conv2d(inter_dims[4], 1, 3, padding=1)\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.adapter1 = nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
    "        self.adapter2 = nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
    "        self.adapter3 = nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):\n",
    "        x = torch.cat([self._expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)\n",
    "\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = self._expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = self._expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = self._expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.out_lay(x)\n",
    "        return x\n",
    "\n",
    "    def _expand(self, tensor, length: int):\n",
    "        return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "    if tensor_list[0].ndim == 3:\n",
    "        max_size = [max(s) for s in zip(*[img.shape for img in tensor_list])]\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "            m[: img.shape[1], :img.shape[2]] = False\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return NestedTensor(tensor, mask)\n",
    "\n",
    "\n",
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device):\n",
    "        cast_tensor = self.tensors.to(device)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            cast_mask = mask.to(device)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone, transformer, num_queries, aux_loss=False):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.transformer = transformer\n",
    "        self.num_queries = num_queries\n",
    "        self.aux_loss = aux_loss\n",
    "\n",
    "        hidden_dim, nheads = transformer.d_model, transformer.nhead\n",
    "        self.bbox_attention = AttentionMap(hidden_dim, hidden_dim, nheads)\n",
    "        self.mask_head = ConvolutionalMaskHead(hidden_dim + nheads, [1024, 512, 256], hidden_dim)\n",
    "\n",
    "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "        self.class_embed = nn.Linear(hidden_dim, 91)\n",
    "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "\n",
    "    def forward(self, samples):\n",
    "        if isinstance(samples, (list, torch.Tensor)):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.backbone(samples)\n",
    "\n",
    "        src, mask = features[-1].decompose()\n",
    "        src_proj = self.input_proj(src)\n",
    "        hs, memory = self.transformer(src_proj, mask, self.query_embed.weight, pos[-1])\n",
    "\n",
    "        outputs_class = self.class_embed(hs)\n",
    "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
    "        out = {\"pred_logits\": outputs_class[-1], \"pred_boxes\": outputs_coord[-1]}\n",
    "        if self.aux_loss:\n",
    "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
    "\n",
    "        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)\n",
    "        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])\n",
    "        outputs_seg_masks = seg_masks.view(samples.tensors.shape[0], self.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
    "\n",
    "        out[\"pred_masks\"] = outputs_seg_masks\n",
    "        return out\n",
    "\n",
    "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
    "        return [{\"pred_logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcess(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
    "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
    "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
    "        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n",
    "        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
    "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
    "\n",
    "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
    "            img_h, img_w = t[0], t[1]\n",
    "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
    "            results[i][\"masks\"] = F.interpolate(results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\").byte()\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def masks_to_boxes(masks):\n",
    "    if masks.numel() == 0:\n",
    "        return torch.zeros((0, 4), device=masks.device)\n",
    "\n",
    "    h, w = masks.shape[-2:]\n",
    "    y = torch.arange(0, h, dtype=torch.float)\n",
    "    x = torch.arange(0, w, dtype=torch.float)\n",
    "    y, x = torch.meshgrid(y, x)\n",
    "\n",
    "    x_mask = (masks * x.unsqueeze(0))\n",
    "    x_max = x_mask.flatten(1).max(-1)[0]\n",
    "    x_min = x_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
    "\n",
    "    y_mask = (masks * y.unsqueeze(0))\n",
    "    y_max = y_mask.flatten(1).max(-1)[0]\n",
    "    y_min = y_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
    "\n",
    "    return torch.stack([x_min, y_min, x_max, y_max], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessPanoptic(nn.Module):\n",
    "    def __init__(self, is_thing_map, threshold=0.85):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.is_thing_map = is_thing_map\n",
    "\n",
    "    def forward(self, outputs, processed_sizes, target_sizes=None):\n",
    "        if target_sizes is None:\n",
    "            target_sizes = processed_sizes\n",
    "        assert len(processed_sizes) == len(target_sizes)\n",
    "        out_logits, raw_masks, raw_boxes = outputs[\"pred_logits\"], outputs[\"pred_masks\"], outputs[\"pred_boxes\"]\n",
    "        assert len(out_logits) == len(raw_masks) == len(target_sizes)\n",
    "        preds = []\n",
    "\n",
    "        def to_tuple(tup):\n",
    "            if isinstance(tup, tuple):\n",
    "                return tup\n",
    "            return tuple(tup.cpu().tolist())\n",
    "\n",
    "        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes):\n",
    "            scores, labels = cur_logits.softmax(-1).max(-1)\n",
    "            keep = labels.ne(outputs[\"pred_logits\"].shape[-1] - 1) & (scores > self.threshold)\n",
    "            cur_scores, cur_classes = cur_logits.softmax(-1).max(-1)\n",
    "            cur_scores = cur_scores[keep]\n",
    "            cur_classes = cur_classes[keep]\n",
    "            cur_masks = cur_masks[keep]\n",
    "            cur_masks = F.interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n",
    "            cur_boxes = box_cxcywh_to_xyxy(cur_boxes[keep])\n",
    "\n",
    "            h, w = cur_masks.shape[-2:]\n",
    "            assert len(cur_boxes) == len(cur_classes)\n",
    "\n",
    "            cur_masks = cur_masks.flatten(1)\n",
    "            stuff_equiv_classes = defaultdict(lambda: [])\n",
    "            for k, label in enumerate(cur_classes):\n",
    "                if not self.is_thing_map[label.item()]:\n",
    "                    stuff_equiv_classes[label.item()].append(k)\n",
    "\n",
    "            def get_ids_area(masks, scores, dedup=False):\n",
    "                m_id = masks.transpose(0, 1).softmax(-1)\n",
    "                if m_id.shape[-1] == 0:\n",
    "                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n",
    "                else:\n",
    "                    m_id = m_id.argmax(-1).view(h, w)\n",
    "                if dedup:\n",
    "                    for equiv in stuff_equiv_classes.values():\n",
    "                        if len(equiv) > 1:\n",
    "                            for eq_id in equiv:\n",
    "                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n",
    "                final_h, final_w = to_tuple(target_size)\n",
    "                seg_img = Image.fromarray(id2rgb(m_id.view(h, w).cpu().numpy()))\n",
    "                seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)\n",
    "                np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes())).view(final_h, final_w, 3).numpy()\n",
    "                m_id = torch.from_numpy(rgb2id(np_seg_img))\n",
    "                area = []\n",
    "                for i in range(len(scores)):\n",
    "                    area.append(m_id.eq(i).sum().item())\n",
    "                return area, seg_img\n",
    "\n",
    "            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n",
    "            if cur_classes.numel() > 0:\n",
    "                while True:\n",
    "                    filtered_small = torch.as_tensor([area[i] <= 4 for i, c in enumerate(cur_classes)], dtype=torch.bool, device=keep.device)\n",
    "                    if filtered_small.any().item():\n",
    "                        cur_scores = cur_scores[~filtered_small]\n",
    "                        cur_classes = cur_classes[~filtered_small]\n",
    "                        cur_masks = cur_masks[~filtered_small]\n",
    "                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)\n",
    "\n",
    "            segments_info = []\n",
    "            for i, a in enumerate(area):\n",
    "                cat = cur_classes[i].item()\n",
    "                segments_info.append({\"id\": i, \"isthing\": self.is_thing_map[cat], \"category_id\": cat, \"area\": a})\n",
    "            del cur_classes\n",
    "\n",
    "            with io.BytesIO() as out:\n",
    "                seg_img.save(out, format=\"PNG\")\n",
    "                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n",
    "            preds.append(predictions)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(inputs, targets, num_boxes):\n",
    "    inputs = inputs.sigmoid().flatten(1)\n",
    "    numerator = 2 * (inputs * targets).sum(1)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
    "    return loss.sum() / num_boxes\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    return loss.mean(1).sum() / num_boxes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
